{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ff3d15",
   "metadata": {},
   "source": [
    "# MNIST Classification using Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab5117",
   "metadata": {},
   "source": [
    "This is an introductory CNN project that classifies MNIST data.\n",
    "\n",
    "The dataset consists of images of size 28 * 28. So, the input to CNN is 28 * 28 image. We will build 2 convolutional layers, each followed by a pooling layer. So, the architecture I will be using is as follows:\n",
    "\n",
    "Input Layer -> ConvLayer_1 -> MaxPooling_1 -> ConvLayer_2 -> MaxPooling_2 -> Flattened Feature Map -> Fully Connected Layer -> Output (Class scores)\n",
    "\n",
    "I will use 3 * 3 Filter size for first ConvLayer and 3 * 3 * 3 (depth of 3 to match 3 channels) with a stride of 1 and no padding (might change based on output) and 2 * 2 Pooling size with stride of 2. Also, each ConvLayer will have 3 filters which generate 3 channels of feature map. Finally, the feature map from last pooling layer is flattened and fed to the fully connected layer. The fully connected layer will perform same operation as in feed-forward neural networks. I will use RELU activation for hidden layer and Softmax for output layer.\n",
    "\n",
    "The size of feature map can be calculated as: \n",
    "((Input Size - Receptive Field Size) + 2 * Zero Padding) / (Stride + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8904c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2404cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ffd729",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f21aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_images_manual(image_path, label_path):\n",
    "    with open (image_path, 'rb') as img_file:\n",
    "        # Read the header\n",
    "        magic, num_images, rows, cols = struct.unpack('>IIII', img_file.read(16))\n",
    "\n",
    "        print(f\"Magic: {magic}, Number of Images: {num_images}, Rows: {rows}, Columns: {cols}\")\n",
    "\n",
    "        # Read the image data\n",
    "        image_data = img_file.read(rows * cols * num_images)\n",
    "        images = torch.frombuffer(image_data, dtype=torch.uint8)\n",
    "\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "\n",
    "    with open (label_path, 'rb') as lbl_file:\n",
    "        # Read the header\n",
    "        magic, num_labels = struct.unpack('>II', lbl_file.read(8))\n",
    "\n",
    "        print(f\"Magic: {magic}, Number of Labels: {num_labels}\")\n",
    "\n",
    "        # Read the label data\n",
    "        label_data = lbl_file.read(num_labels)\n",
    "        labels = torch.frombuffer(label_data, dtype=torch.uint8)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc57c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some random images\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def display_images(images, labels):\n",
    "    cols = 4\n",
    "    rows = math.ceil(len(images) / cols)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    index = 0\n",
    "\n",
    "    for x in zip(images, labels):\n",
    "        image = x[0]\n",
    "        label = x[1]\n",
    "        plt.subplot(rows, cols, index + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5caef15",
   "metadata": {},
   "source": [
    "## Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30bc8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4434f",
   "metadata": {},
   "source": [
    "## Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d426410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Convolution Layer 1\n",
    "        self.conv_layer_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu_1 = nn.ReLU()\n",
    " \n",
    "        # Max Pooling 1\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Convolution Layer 2\n",
    "        self.conv_layer_2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "\n",
    "        # Max Pooling 2\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Flattening layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        # Fully Connected 1\n",
    "        self.fc_1 = nn.Linear(in_features=3 * 4 * 4, out_features=24)\n",
    "        self.fc_1_relu = nn.ReLU()\n",
    "\n",
    "        # Fully Connected 2\n",
    "        self.fc_2 = nn.Linear(in_features=24, out_features=10)\n",
    "\n",
    "        # CrossEntropyLoss implicitly implements LogSoftmax, so don't need this activation.\n",
    "        # self.fc_2_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Convolution 1\n",
    "        out = self.conv_layer_1(X)\n",
    "        out = self.relu_1(out)\n",
    "\n",
    "        # Max Pooling 1\n",
    "        out = self.max_pool_1(out)\n",
    "\n",
    "        # Convolution 2\n",
    "        out = self.conv_layer_2(out)\n",
    "        out = self.relu_2(out)\n",
    "\n",
    "        # Max Pooling 2\n",
    "        out = self.max_pool_2(out)\n",
    "\n",
    "        # Flatten\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Fully Connected 1\n",
    "        out = self.fc_1(out)\n",
    "        out = self.fc_1_relu(out)\n",
    "\n",
    "        # Fully Connected 2\n",
    "        out = self.fc_2(out)\n",
    "        # out = self.fc_2_softmax(out) // CrossEntropyLoss implicitly implements LogSoftmax\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f8fcc",
   "metadata": {},
   "source": [
    "## Initialize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8fe0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "total_epoch = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07831cf3",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f9f3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic: 2051, Number of Images: 60000, Rows: 28, Columns: 28\n",
      "Magic: 2049, Number of Labels: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sujan\\AppData\\Local\\Temp\\ipykernel_22452\\2397727395.py:10: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:1567.)\n",
      "  images = torch.frombuffer(image_data, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist_images_manual(\"dataset/train-images.idx3-ubyte\", \"dataset/train-labels.idx1-ubyte\")\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01244d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([60000, 28, 28])\n",
      "y_train shape: torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c735f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(X_train, y_train)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c42bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e5dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab2bc7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [1/937], Loss: 15.4330 \n",
      "\n",
      "\n",
      "Epoch [1/100], Step [101/937], Loss: 2.3277 \n",
      "\n",
      "\n",
      "Epoch [1/100], Step [201/937], Loss: 2.2789 \n",
      "\n",
      "\n",
      "Epoch [1/100], Step [301/937], Loss: 2.2569 \n",
      "\n",
      "\n",
      "Epoch [1/100], Step [401/937], Loss: 2.1104 \n",
      "\n",
      "\n",
      "Epoch [1/100], Step [501/937], Loss: 1.8487 \n",
      "\n",
      "\n",
      "Epoch [1/100], Step [601/937], Loss: 1.7049 \n",
      "\n",
      "\n",
      "Epoch [1/100], Step [701/937], Loss: 1.2059 \n",
      "\n",
      "\n",
      "Epoch [1/100], Step [801/937], Loss: 1.3471 \n",
      "\n",
      "\n",
      "Epoch [1/100], Step [901/937], Loss: 0.9729 \n",
      "\n",
      "\n",
      "Epoch [2/100], Step [1/937], Loss: 0.7683 \n",
      "\n",
      "\n",
      "Epoch [2/100], Step [101/937], Loss: 0.9307 \n",
      "\n",
      "\n",
      "Epoch [2/100], Step [201/937], Loss: 0.6847 \n",
      "\n",
      "\n",
      "Epoch [2/100], Step [301/937], Loss: 0.5848 \n",
      "\n",
      "\n",
      "Epoch [2/100], Step [401/937], Loss: 0.5332 \n",
      "\n",
      "\n",
      "Epoch [2/100], Step [501/937], Loss: 0.4474 \n",
      "\n",
      "\n",
      "Epoch [2/100], Step [601/937], Loss: 0.5922 \n",
      "\n",
      "\n",
      "Epoch [2/100], Step [701/937], Loss: 0.5172 \n",
      "\n",
      "\n",
      "Epoch [2/100], Step [801/937], Loss: 0.2761 \n",
      "\n",
      "\n",
      "Epoch [2/100], Step [901/937], Loss: 0.4362 \n",
      "\n",
      "\n",
      "Epoch [3/100], Step [1/937], Loss: 0.5034 \n",
      "\n",
      "\n",
      "Epoch [3/100], Step [101/937], Loss: 0.5939 \n",
      "\n",
      "\n",
      "Epoch [3/100], Step [201/937], Loss: 0.3620 \n",
      "\n",
      "\n",
      "Epoch [3/100], Step [301/937], Loss: 0.4744 \n",
      "\n",
      "\n",
      "Epoch [3/100], Step [401/937], Loss: 0.3602 \n",
      "\n",
      "\n",
      "Epoch [3/100], Step [501/937], Loss: 0.4815 \n",
      "\n",
      "\n",
      "Epoch [3/100], Step [601/937], Loss: 0.3225 \n",
      "\n",
      "\n",
      "Epoch [3/100], Step [701/937], Loss: 0.3333 \n",
      "\n",
      "\n",
      "Epoch [3/100], Step [801/937], Loss: 0.2867 \n",
      "\n",
      "\n",
      "Epoch [3/100], Step [901/937], Loss: 0.3599 \n",
      "\n",
      "\n",
      "Epoch [4/100], Step [1/937], Loss: 0.3825 \n",
      "\n",
      "\n",
      "Epoch [4/100], Step [101/937], Loss: 0.2062 \n",
      "\n",
      "\n",
      "Epoch [4/100], Step [201/937], Loss: 0.3015 \n",
      "\n",
      "\n",
      "Epoch [4/100], Step [301/937], Loss: 0.3881 \n",
      "\n",
      "\n",
      "Epoch [4/100], Step [401/937], Loss: 0.3746 \n",
      "\n",
      "\n",
      "Epoch [4/100], Step [501/937], Loss: 0.2301 \n",
      "\n",
      "\n",
      "Epoch [4/100], Step [601/937], Loss: 0.2844 \n",
      "\n",
      "\n",
      "Epoch [4/100], Step [701/937], Loss: 0.5110 \n",
      "\n",
      "\n",
      "Epoch [4/100], Step [801/937], Loss: 0.3880 \n",
      "\n",
      "\n",
      "Epoch [4/100], Step [901/937], Loss: 0.2423 \n",
      "\n",
      "\n",
      "Epoch [5/100], Step [1/937], Loss: 0.3384 \n",
      "\n",
      "\n",
      "Epoch [5/100], Step [101/937], Loss: 0.3493 \n",
      "\n",
      "\n",
      "Epoch [5/100], Step [201/937], Loss: 0.2196 \n",
      "\n",
      "\n",
      "Epoch [5/100], Step [301/937], Loss: 0.4313 \n",
      "\n",
      "\n",
      "Epoch [5/100], Step [401/937], Loss: 0.1920 \n",
      "\n",
      "\n",
      "Epoch [5/100], Step [501/937], Loss: 0.2950 \n",
      "\n",
      "\n",
      "Epoch [5/100], Step [601/937], Loss: 0.2607 \n",
      "\n",
      "\n",
      "Epoch [5/100], Step [701/937], Loss: 0.2546 \n",
      "\n",
      "\n",
      "Epoch [5/100], Step [801/937], Loss: 0.5907 \n",
      "\n",
      "\n",
      "Epoch [5/100], Step [901/937], Loss: 0.2282 \n",
      "\n",
      "\n",
      "Epoch [6/100], Step [1/937], Loss: 0.2433 \n",
      "\n",
      "\n",
      "Epoch [6/100], Step [101/937], Loss: 0.1697 \n",
      "\n",
      "\n",
      "Epoch [6/100], Step [201/937], Loss: 0.0824 \n",
      "\n",
      "\n",
      "Epoch [6/100], Step [301/937], Loss: 0.2209 \n",
      "\n",
      "\n",
      "Epoch [6/100], Step [401/937], Loss: 0.1528 \n",
      "\n",
      "\n",
      "Epoch [6/100], Step [501/937], Loss: 0.1195 \n",
      "\n",
      "\n",
      "Epoch [6/100], Step [601/937], Loss: 0.1784 \n",
      "\n",
      "\n",
      "Epoch [6/100], Step [701/937], Loss: 0.2099 \n",
      "\n",
      "\n",
      "Epoch [6/100], Step [801/937], Loss: 0.2244 \n",
      "\n",
      "\n",
      "Epoch [6/100], Step [901/937], Loss: 0.1759 \n",
      "\n",
      "\n",
      "Epoch [7/100], Step [1/937], Loss: 0.2751 \n",
      "\n",
      "\n",
      "Epoch [7/100], Step [101/937], Loss: 0.3342 \n",
      "\n",
      "\n",
      "Epoch [7/100], Step [201/937], Loss: 0.2214 \n",
      "\n",
      "\n",
      "Epoch [7/100], Step [301/937], Loss: 0.4659 \n",
      "\n",
      "\n",
      "Epoch [7/100], Step [401/937], Loss: 0.2268 \n",
      "\n",
      "\n",
      "Epoch [7/100], Step [501/937], Loss: 0.4719 \n",
      "\n",
      "\n",
      "Epoch [7/100], Step [601/937], Loss: 0.1490 \n",
      "\n",
      "\n",
      "Epoch [7/100], Step [701/937], Loss: 0.1743 \n",
      "\n",
      "\n",
      "Epoch [7/100], Step [801/937], Loss: 0.1411 \n",
      "\n",
      "\n",
      "Epoch [7/100], Step [901/937], Loss: 0.1245 \n",
      "\n",
      "\n",
      "Epoch [8/100], Step [1/937], Loss: 0.2257 \n",
      "\n",
      "\n",
      "Epoch [8/100], Step [101/937], Loss: 0.0693 \n",
      "\n",
      "\n",
      "Epoch [8/100], Step [201/937], Loss: 0.1399 \n",
      "\n",
      "\n",
      "Epoch [8/100], Step [301/937], Loss: 0.1223 \n",
      "\n",
      "\n",
      "Epoch [8/100], Step [401/937], Loss: 0.1260 \n",
      "\n",
      "\n",
      "Epoch [8/100], Step [501/937], Loss: 0.1725 \n",
      "\n",
      "\n",
      "Epoch [8/100], Step [601/937], Loss: 0.4313 \n",
      "\n",
      "\n",
      "Epoch [8/100], Step [701/937], Loss: 0.1037 \n",
      "\n",
      "\n",
      "Epoch [8/100], Step [801/937], Loss: 0.3273 \n",
      "\n",
      "\n",
      "Epoch [8/100], Step [901/937], Loss: 0.1236 \n",
      "\n",
      "\n",
      "Epoch [9/100], Step [1/937], Loss: 0.1926 \n",
      "\n",
      "\n",
      "Epoch [9/100], Step [101/937], Loss: 0.2261 \n",
      "\n",
      "\n",
      "Epoch [9/100], Step [201/937], Loss: 0.2089 \n",
      "\n",
      "\n",
      "Epoch [9/100], Step [301/937], Loss: 0.2501 \n",
      "\n",
      "\n",
      "Epoch [9/100], Step [401/937], Loss: 0.1215 \n",
      "\n",
      "\n",
      "Epoch [9/100], Step [501/937], Loss: 0.3807 \n",
      "\n",
      "\n",
      "Epoch [9/100], Step [601/937], Loss: 0.1305 \n",
      "\n",
      "\n",
      "Epoch [9/100], Step [701/937], Loss: 0.1567 \n",
      "\n",
      "\n",
      "Epoch [9/100], Step [801/937], Loss: 0.1131 \n",
      "\n",
      "\n",
      "Epoch [9/100], Step [901/937], Loss: 0.2399 \n",
      "\n",
      "\n",
      "Epoch [10/100], Step [1/937], Loss: 0.2599 \n",
      "\n",
      "\n",
      "Epoch [10/100], Step [101/937], Loss: 0.1132 \n",
      "\n",
      "\n",
      "Epoch [10/100], Step [201/937], Loss: 0.1420 \n",
      "\n",
      "\n",
      "Epoch [10/100], Step [301/937], Loss: 0.2932 \n",
      "\n",
      "\n",
      "Epoch [10/100], Step [401/937], Loss: 0.2887 \n",
      "\n",
      "\n",
      "Epoch [10/100], Step [501/937], Loss: 0.1200 \n",
      "\n",
      "\n",
      "Epoch [10/100], Step [601/937], Loss: 0.0865 \n",
      "\n",
      "\n",
      "Epoch [10/100], Step [701/937], Loss: 0.2081 \n",
      "\n",
      "\n",
      "Epoch [10/100], Step [801/937], Loss: 0.1664 \n",
      "\n",
      "\n",
      "Epoch [10/100], Step [901/937], Loss: 0.0809 \n",
      "\n",
      "\n",
      "Epoch [11/100], Step [1/937], Loss: 0.1345 \n",
      "\n",
      "\n",
      "Epoch [11/100], Step [101/937], Loss: 0.1843 \n",
      "\n",
      "\n",
      "Epoch [11/100], Step [201/937], Loss: 0.1230 \n",
      "\n",
      "\n",
      "Epoch [11/100], Step [301/937], Loss: 0.2622 \n",
      "\n",
      "\n",
      "Epoch [11/100], Step [401/937], Loss: 0.0293 \n",
      "\n",
      "\n",
      "Epoch [11/100], Step [501/937], Loss: 0.1670 \n",
      "\n",
      "\n",
      "Epoch [11/100], Step [601/937], Loss: 0.1015 \n",
      "\n",
      "\n",
      "Epoch [11/100], Step [701/937], Loss: 0.1169 \n",
      "\n",
      "\n",
      "Epoch [11/100], Step [801/937], Loss: 0.2366 \n",
      "\n",
      "\n",
      "Epoch [11/100], Step [901/937], Loss: 0.0854 \n",
      "\n",
      "\n",
      "Epoch [12/100], Step [1/937], Loss: 0.1244 \n",
      "\n",
      "\n",
      "Epoch [12/100], Step [101/937], Loss: 0.2607 \n",
      "\n",
      "\n",
      "Epoch [12/100], Step [201/937], Loss: 0.1514 \n",
      "\n",
      "\n",
      "Epoch [12/100], Step [301/937], Loss: 0.4172 \n",
      "\n",
      "\n",
      "Epoch [12/100], Step [401/937], Loss: 0.1307 \n",
      "\n",
      "\n",
      "Epoch [12/100], Step [501/937], Loss: 0.1743 \n",
      "\n",
      "\n",
      "Epoch [12/100], Step [601/937], Loss: 0.1450 \n",
      "\n",
      "\n",
      "Epoch [12/100], Step [701/937], Loss: 0.1973 \n",
      "\n",
      "\n",
      "Epoch [12/100], Step [801/937], Loss: 0.0877 \n",
      "\n",
      "\n",
      "Epoch [12/100], Step [901/937], Loss: 0.0943 \n",
      "\n",
      "\n",
      "Epoch [13/100], Step [1/937], Loss: 0.1431 \n",
      "\n",
      "\n",
      "Epoch [13/100], Step [101/937], Loss: 0.1089 \n",
      "\n",
      "\n",
      "Epoch [13/100], Step [201/937], Loss: 0.0901 \n",
      "\n",
      "\n",
      "Epoch [13/100], Step [301/937], Loss: 0.3933 \n",
      "\n",
      "\n",
      "Epoch [13/100], Step [401/937], Loss: 0.2134 \n",
      "\n",
      "\n",
      "Epoch [13/100], Step [501/937], Loss: 0.1527 \n",
      "\n",
      "\n",
      "Epoch [13/100], Step [601/937], Loss: 0.1883 \n",
      "\n",
      "\n",
      "Epoch [13/100], Step [701/937], Loss: 0.1766 \n",
      "\n",
      "\n",
      "Epoch [13/100], Step [801/937], Loss: 0.0389 \n",
      "\n",
      "\n",
      "Epoch [13/100], Step [901/937], Loss: 0.2900 \n",
      "\n",
      "\n",
      "Epoch [14/100], Step [1/937], Loss: 0.1800 \n",
      "\n",
      "\n",
      "Epoch [14/100], Step [101/937], Loss: 0.2269 \n",
      "\n",
      "\n",
      "Epoch [14/100], Step [201/937], Loss: 0.2151 \n",
      "\n",
      "\n",
      "Epoch [14/100], Step [301/937], Loss: 0.2269 \n",
      "\n",
      "\n",
      "Epoch [14/100], Step [401/937], Loss: 0.0561 \n",
      "\n",
      "\n",
      "Epoch [14/100], Step [501/937], Loss: 0.0869 \n",
      "\n",
      "\n",
      "Epoch [14/100], Step [601/937], Loss: 0.2815 \n",
      "\n",
      "\n",
      "Epoch [14/100], Step [701/937], Loss: 0.3144 \n",
      "\n",
      "\n",
      "Epoch [14/100], Step [801/937], Loss: 0.3684 \n",
      "\n",
      "\n",
      "Epoch [14/100], Step [901/937], Loss: 0.1237 \n",
      "\n",
      "\n",
      "Epoch [15/100], Step [1/937], Loss: 0.0594 \n",
      "\n",
      "\n",
      "Epoch [15/100], Step [101/937], Loss: 0.0315 \n",
      "\n",
      "\n",
      "Epoch [15/100], Step [201/937], Loss: 0.1152 \n",
      "\n",
      "\n",
      "Epoch [15/100], Step [301/937], Loss: 0.0388 \n",
      "\n",
      "\n",
      "Epoch [15/100], Step [401/937], Loss: 0.1022 \n",
      "\n",
      "\n",
      "Epoch [15/100], Step [501/937], Loss: 0.1099 \n",
      "\n",
      "\n",
      "Epoch [15/100], Step [601/937], Loss: 0.1853 \n",
      "\n",
      "\n",
      "Epoch [15/100], Step [701/937], Loss: 0.1462 \n",
      "\n",
      "\n",
      "Epoch [15/100], Step [801/937], Loss: 0.0744 \n",
      "\n",
      "\n",
      "Epoch [15/100], Step [901/937], Loss: 0.0647 \n",
      "\n",
      "\n",
      "Epoch [16/100], Step [1/937], Loss: 0.0813 \n",
      "\n",
      "\n",
      "Epoch [16/100], Step [101/937], Loss: 0.1446 \n",
      "\n",
      "\n",
      "Epoch [16/100], Step [201/937], Loss: 0.1617 \n",
      "\n",
      "\n",
      "Epoch [16/100], Step [301/937], Loss: 0.0855 \n",
      "\n",
      "\n",
      "Epoch [16/100], Step [401/937], Loss: 0.1743 \n",
      "\n",
      "\n",
      "Epoch [16/100], Step [501/937], Loss: 0.1247 \n",
      "\n",
      "\n",
      "Epoch [16/100], Step [601/937], Loss: 0.2006 \n",
      "\n",
      "\n",
      "Epoch [16/100], Step [701/937], Loss: 0.1902 \n",
      "\n",
      "\n",
      "Epoch [16/100], Step [801/937], Loss: 0.1453 \n",
      "\n",
      "\n",
      "Epoch [16/100], Step [901/937], Loss: 0.1510 \n",
      "\n",
      "\n",
      "Epoch [17/100], Step [1/937], Loss: 0.1562 \n",
      "\n",
      "\n",
      "Epoch [17/100], Step [101/937], Loss: 0.1437 \n",
      "\n",
      "\n",
      "Epoch [17/100], Step [201/937], Loss: 0.1350 \n",
      "\n",
      "\n",
      "Epoch [17/100], Step [301/937], Loss: 0.0608 \n",
      "\n",
      "\n",
      "Epoch [17/100], Step [401/937], Loss: 0.1289 \n",
      "\n",
      "\n",
      "Epoch [17/100], Step [501/937], Loss: 0.1439 \n",
      "\n",
      "\n",
      "Epoch [17/100], Step [601/937], Loss: 0.1336 \n",
      "\n",
      "\n",
      "Epoch [17/100], Step [701/937], Loss: 0.1628 \n",
      "\n",
      "\n",
      "Epoch [17/100], Step [801/937], Loss: 0.0401 \n",
      "\n",
      "\n",
      "Epoch [17/100], Step [901/937], Loss: 0.1008 \n",
      "\n",
      "\n",
      "Epoch [18/100], Step [1/937], Loss: 0.1305 \n",
      "\n",
      "\n",
      "Epoch [18/100], Step [101/937], Loss: 0.0490 \n",
      "\n",
      "\n",
      "Epoch [18/100], Step [201/937], Loss: 0.0913 \n",
      "\n",
      "\n",
      "Epoch [18/100], Step [301/937], Loss: 0.1005 \n",
      "\n",
      "\n",
      "Epoch [18/100], Step [401/937], Loss: 0.1514 \n",
      "\n",
      "\n",
      "Epoch [18/100], Step [501/937], Loss: 0.1438 \n",
      "\n",
      "\n",
      "Epoch [18/100], Step [601/937], Loss: 0.0869 \n",
      "\n",
      "\n",
      "Epoch [18/100], Step [701/937], Loss: 0.1529 \n",
      "\n",
      "\n",
      "Epoch [18/100], Step [801/937], Loss: 0.1030 \n",
      "\n",
      "\n",
      "Epoch [18/100], Step [901/937], Loss: 0.1010 \n",
      "\n",
      "\n",
      "Epoch [19/100], Step [1/937], Loss: 0.2921 \n",
      "\n",
      "\n",
      "Epoch [19/100], Step [101/937], Loss: 0.0736 \n",
      "\n",
      "\n",
      "Epoch [19/100], Step [201/937], Loss: 0.2173 \n",
      "\n",
      "\n",
      "Epoch [19/100], Step [301/937], Loss: 0.1559 \n",
      "\n",
      "\n",
      "Epoch [19/100], Step [401/937], Loss: 0.0632 \n",
      "\n",
      "\n",
      "Epoch [19/100], Step [501/937], Loss: 0.1995 \n",
      "\n",
      "\n",
      "Epoch [19/100], Step [601/937], Loss: 0.2756 \n",
      "\n",
      "\n",
      "Epoch [19/100], Step [701/937], Loss: 0.1075 \n",
      "\n",
      "\n",
      "Epoch [19/100], Step [801/937], Loss: 0.1564 \n",
      "\n",
      "\n",
      "Epoch [19/100], Step [901/937], Loss: 0.2315 \n",
      "\n",
      "\n",
      "Epoch [20/100], Step [1/937], Loss: 0.1470 \n",
      "\n",
      "\n",
      "Epoch [20/100], Step [101/937], Loss: 0.0296 \n",
      "\n",
      "\n",
      "Epoch [20/100], Step [201/937], Loss: 0.0235 \n",
      "\n",
      "\n",
      "Epoch [20/100], Step [301/937], Loss: 0.2397 \n",
      "\n",
      "\n",
      "Epoch [20/100], Step [401/937], Loss: 0.2784 \n",
      "\n",
      "\n",
      "Epoch [20/100], Step [501/937], Loss: 0.0729 \n",
      "\n",
      "\n",
      "Epoch [20/100], Step [601/937], Loss: 0.2273 \n",
      "\n",
      "\n",
      "Epoch [20/100], Step [701/937], Loss: 0.0559 \n",
      "\n",
      "\n",
      "Epoch [20/100], Step [801/937], Loss: 0.1046 \n",
      "\n",
      "\n",
      "Epoch [20/100], Step [901/937], Loss: 0.1117 \n",
      "\n",
      "\n",
      "Epoch [21/100], Step [1/937], Loss: 0.0588 \n",
      "\n",
      "\n",
      "Epoch [21/100], Step [101/937], Loss: 0.0564 \n",
      "\n",
      "\n",
      "Epoch [21/100], Step [201/937], Loss: 0.0455 \n",
      "\n",
      "\n",
      "Epoch [21/100], Step [301/937], Loss: 0.0555 \n",
      "\n",
      "\n",
      "Epoch [21/100], Step [401/937], Loss: 0.1941 \n",
      "\n",
      "\n",
      "Epoch [21/100], Step [501/937], Loss: 0.1461 \n",
      "\n",
      "\n",
      "Epoch [21/100], Step [601/937], Loss: 0.1349 \n",
      "\n",
      "\n",
      "Epoch [21/100], Step [701/937], Loss: 0.3204 \n",
      "\n",
      "\n",
      "Epoch [21/100], Step [801/937], Loss: 0.0921 \n",
      "\n",
      "\n",
      "Epoch [21/100], Step [901/937], Loss: 0.2512 \n",
      "\n",
      "\n",
      "Epoch [22/100], Step [1/937], Loss: 0.3835 \n",
      "\n",
      "\n",
      "Epoch [22/100], Step [101/937], Loss: 0.0171 \n",
      "\n",
      "\n",
      "Epoch [22/100], Step [201/937], Loss: 0.0566 \n",
      "\n",
      "\n",
      "Epoch [22/100], Step [301/937], Loss: 0.0828 \n",
      "\n",
      "\n",
      "Epoch [22/100], Step [401/937], Loss: 0.0608 \n",
      "\n",
      "\n",
      "Epoch [22/100], Step [501/937], Loss: 0.0860 \n",
      "\n",
      "\n",
      "Epoch [22/100], Step [601/937], Loss: 0.0838 \n",
      "\n",
      "\n",
      "Epoch [22/100], Step [701/937], Loss: 0.0288 \n",
      "\n",
      "\n",
      "Epoch [22/100], Step [801/937], Loss: 0.0241 \n",
      "\n",
      "\n",
      "Epoch [22/100], Step [901/937], Loss: 0.2141 \n",
      "\n",
      "\n",
      "Epoch [23/100], Step [1/937], Loss: 0.1478 \n",
      "\n",
      "\n",
      "Epoch [23/100], Step [101/937], Loss: 0.1848 \n",
      "\n",
      "\n",
      "Epoch [23/100], Step [201/937], Loss: 0.0526 \n",
      "\n",
      "\n",
      "Epoch [23/100], Step [301/937], Loss: 0.0904 \n",
      "\n",
      "\n",
      "Epoch [23/100], Step [401/937], Loss: 0.0666 \n",
      "\n",
      "\n",
      "Epoch [23/100], Step [501/937], Loss: 0.0593 \n",
      "\n",
      "\n",
      "Epoch [23/100], Step [601/937], Loss: 0.1882 \n",
      "\n",
      "\n",
      "Epoch [23/100], Step [701/937], Loss: 0.0344 \n",
      "\n",
      "\n",
      "Epoch [23/100], Step [801/937], Loss: 0.1255 \n",
      "\n",
      "\n",
      "Epoch [23/100], Step [901/937], Loss: 0.0204 \n",
      "\n",
      "\n",
      "Epoch [24/100], Step [1/937], Loss: 0.0333 \n",
      "\n",
      "\n",
      "Epoch [24/100], Step [101/937], Loss: 0.1431 \n",
      "\n",
      "\n",
      "Epoch [24/100], Step [201/937], Loss: 0.1073 \n",
      "\n",
      "\n",
      "Epoch [24/100], Step [301/937], Loss: 0.1687 \n",
      "\n",
      "\n",
      "Epoch [24/100], Step [401/937], Loss: 0.1356 \n",
      "\n",
      "\n",
      "Epoch [24/100], Step [501/937], Loss: 0.1298 \n",
      "\n",
      "\n",
      "Epoch [24/100], Step [601/937], Loss: 0.3021 \n",
      "\n",
      "\n",
      "Epoch [24/100], Step [701/937], Loss: 0.0612 \n",
      "\n",
      "\n",
      "Epoch [24/100], Step [801/937], Loss: 0.1659 \n",
      "\n",
      "\n",
      "Epoch [24/100], Step [901/937], Loss: 0.1596 \n",
      "\n",
      "\n",
      "Epoch [25/100], Step [1/937], Loss: 0.0495 \n",
      "\n",
      "\n",
      "Epoch [25/100], Step [101/937], Loss: 0.1081 \n",
      "\n",
      "\n",
      "Epoch [25/100], Step [201/937], Loss: 0.1488 \n",
      "\n",
      "\n",
      "Epoch [25/100], Step [301/937], Loss: 0.0462 \n",
      "\n",
      "\n",
      "Epoch [25/100], Step [401/937], Loss: 0.0830 \n",
      "\n",
      "\n",
      "Epoch [25/100], Step [501/937], Loss: 0.1525 \n",
      "\n",
      "\n",
      "Epoch [25/100], Step [601/937], Loss: 0.1676 \n",
      "\n",
      "\n",
      "Epoch [25/100], Step [701/937], Loss: 0.1309 \n",
      "\n",
      "\n",
      "Epoch [25/100], Step [801/937], Loss: 0.2676 \n",
      "\n",
      "\n",
      "Epoch [25/100], Step [901/937], Loss: 0.1817 \n",
      "\n",
      "\n",
      "Epoch [26/100], Step [1/937], Loss: 0.0400 \n",
      "\n",
      "\n",
      "Epoch [26/100], Step [101/937], Loss: 0.0968 \n",
      "\n",
      "\n",
      "Epoch [26/100], Step [201/937], Loss: 0.1641 \n",
      "\n",
      "\n",
      "Epoch [26/100], Step [301/937], Loss: 0.2659 \n",
      "\n",
      "\n",
      "Epoch [26/100], Step [401/937], Loss: 0.0926 \n",
      "\n",
      "\n",
      "Epoch [26/100], Step [501/937], Loss: 0.1447 \n",
      "\n",
      "\n",
      "Epoch [26/100], Step [601/937], Loss: 0.1650 \n",
      "\n",
      "\n",
      "Epoch [26/100], Step [701/937], Loss: 0.1752 \n",
      "\n",
      "\n",
      "Epoch [26/100], Step [801/937], Loss: 0.0352 \n",
      "\n",
      "\n",
      "Epoch [26/100], Step [901/937], Loss: 0.0952 \n",
      "\n",
      "\n",
      "Epoch [27/100], Step [1/937], Loss: 0.1676 \n",
      "\n",
      "\n",
      "Epoch [27/100], Step [101/937], Loss: 0.1275 \n",
      "\n",
      "\n",
      "Epoch [27/100], Step [201/937], Loss: 0.1819 \n",
      "\n",
      "\n",
      "Epoch [27/100], Step [301/937], Loss: 0.1631 \n",
      "\n",
      "\n",
      "Epoch [27/100], Step [401/937], Loss: 0.1994 \n",
      "\n",
      "\n",
      "Epoch [27/100], Step [501/937], Loss: 0.0670 \n",
      "\n",
      "\n",
      "Epoch [27/100], Step [601/937], Loss: 0.1216 \n",
      "\n",
      "\n",
      "Epoch [27/100], Step [701/937], Loss: 0.1034 \n",
      "\n",
      "\n",
      "Epoch [27/100], Step [801/937], Loss: 0.0881 \n",
      "\n",
      "\n",
      "Epoch [27/100], Step [901/937], Loss: 0.0355 \n",
      "\n",
      "\n",
      "Epoch [28/100], Step [1/937], Loss: 0.1865 \n",
      "\n",
      "\n",
      "Epoch [28/100], Step [101/937], Loss: 0.0505 \n",
      "\n",
      "\n",
      "Epoch [28/100], Step [201/937], Loss: 0.1273 \n",
      "\n",
      "\n",
      "Epoch [28/100], Step [301/937], Loss: 0.0492 \n",
      "\n",
      "\n",
      "Epoch [28/100], Step [401/937], Loss: 0.0320 \n",
      "\n",
      "\n",
      "Epoch [28/100], Step [501/937], Loss: 0.0688 \n",
      "\n",
      "\n",
      "Epoch [28/100], Step [601/937], Loss: 0.2091 \n",
      "\n",
      "\n",
      "Epoch [28/100], Step [701/937], Loss: 0.1238 \n",
      "\n",
      "\n",
      "Epoch [28/100], Step [801/937], Loss: 0.1529 \n",
      "\n",
      "\n",
      "Epoch [28/100], Step [901/937], Loss: 0.2354 \n",
      "\n",
      "\n",
      "Epoch [29/100], Step [1/937], Loss: 0.0892 \n",
      "\n",
      "\n",
      "Epoch [29/100], Step [101/937], Loss: 0.0576 \n",
      "\n",
      "\n",
      "Epoch [29/100], Step [201/937], Loss: 0.0963 \n",
      "\n",
      "\n",
      "Epoch [29/100], Step [301/937], Loss: 0.0614 \n",
      "\n",
      "\n",
      "Epoch [29/100], Step [401/937], Loss: 0.1354 \n",
      "\n",
      "\n",
      "Epoch [29/100], Step [501/937], Loss: 0.0618 \n",
      "\n",
      "\n",
      "Epoch [29/100], Step [601/937], Loss: 0.1680 \n",
      "\n",
      "\n",
      "Epoch [29/100], Step [701/937], Loss: 0.1582 \n",
      "\n",
      "\n",
      "Epoch [29/100], Step [801/937], Loss: 0.1443 \n",
      "\n",
      "\n",
      "Epoch [29/100], Step [901/937], Loss: 0.0346 \n",
      "\n",
      "\n",
      "Epoch [30/100], Step [1/937], Loss: 0.0252 \n",
      "\n",
      "\n",
      "Epoch [30/100], Step [101/937], Loss: 0.1499 \n",
      "\n",
      "\n",
      "Epoch [30/100], Step [201/937], Loss: 0.0396 \n",
      "\n",
      "\n",
      "Epoch [30/100], Step [301/937], Loss: 0.1207 \n",
      "\n",
      "\n",
      "Epoch [30/100], Step [401/937], Loss: 0.0927 \n",
      "\n",
      "\n",
      "Epoch [30/100], Step [501/937], Loss: 0.0690 \n",
      "\n",
      "\n",
      "Epoch [30/100], Step [601/937], Loss: 0.0384 \n",
      "\n",
      "\n",
      "Epoch [30/100], Step [701/937], Loss: 0.1337 \n",
      "\n",
      "\n",
      "Epoch [30/100], Step [801/937], Loss: 0.1416 \n",
      "\n",
      "\n",
      "Epoch [30/100], Step [901/937], Loss: 0.2035 \n",
      "\n",
      "\n",
      "Epoch [31/100], Step [1/937], Loss: 0.0332 \n",
      "\n",
      "\n",
      "Epoch [31/100], Step [101/937], Loss: 0.0212 \n",
      "\n",
      "\n",
      "Epoch [31/100], Step [201/937], Loss: 0.0233 \n",
      "\n",
      "\n",
      "Epoch [31/100], Step [301/937], Loss: 0.1161 \n",
      "\n",
      "\n",
      "Epoch [31/100], Step [401/937], Loss: 0.0633 \n",
      "\n",
      "\n",
      "Epoch [31/100], Step [501/937], Loss: 0.2365 \n",
      "\n",
      "\n",
      "Epoch [31/100], Step [601/937], Loss: 0.0360 \n",
      "\n",
      "\n",
      "Epoch [31/100], Step [701/937], Loss: 0.0675 \n",
      "\n",
      "\n",
      "Epoch [31/100], Step [801/937], Loss: 0.0560 \n",
      "\n",
      "\n",
      "Epoch [31/100], Step [901/937], Loss: 0.3118 \n",
      "\n",
      "\n",
      "Epoch [32/100], Step [1/937], Loss: 0.2541 \n",
      "\n",
      "\n",
      "Epoch [32/100], Step [101/937], Loss: 0.0391 \n",
      "\n",
      "\n",
      "Epoch [32/100], Step [201/937], Loss: 0.0677 \n",
      "\n",
      "\n",
      "Epoch [32/100], Step [301/937], Loss: 0.0804 \n",
      "\n",
      "\n",
      "Epoch [32/100], Step [401/937], Loss: 0.0840 \n",
      "\n",
      "\n",
      "Epoch [32/100], Step [501/937], Loss: 0.1632 \n",
      "\n",
      "\n",
      "Epoch [32/100], Step [601/937], Loss: 0.0582 \n",
      "\n",
      "\n",
      "Epoch [32/100], Step [701/937], Loss: 0.0626 \n",
      "\n",
      "\n",
      "Epoch [32/100], Step [801/937], Loss: 0.1156 \n",
      "\n",
      "\n",
      "Epoch [32/100], Step [901/937], Loss: 0.0936 \n",
      "\n",
      "\n",
      "Epoch [33/100], Step [1/937], Loss: 0.1233 \n",
      "\n",
      "\n",
      "Epoch [33/100], Step [101/937], Loss: 0.1886 \n",
      "\n",
      "\n",
      "Epoch [33/100], Step [201/937], Loss: 0.0478 \n",
      "\n",
      "\n",
      "Epoch [33/100], Step [301/937], Loss: 0.0224 \n",
      "\n",
      "\n",
      "Epoch [33/100], Step [401/937], Loss: 0.0531 \n",
      "\n",
      "\n",
      "Epoch [33/100], Step [501/937], Loss: 0.1870 \n",
      "\n",
      "\n",
      "Epoch [33/100], Step [601/937], Loss: 0.1051 \n",
      "\n",
      "\n",
      "Epoch [33/100], Step [701/937], Loss: 0.1020 \n",
      "\n",
      "\n",
      "Epoch [33/100], Step [801/937], Loss: 0.0482 \n",
      "\n",
      "\n",
      "Epoch [33/100], Step [901/937], Loss: 0.0410 \n",
      "\n",
      "\n",
      "Epoch [34/100], Step [1/937], Loss: 0.2218 \n",
      "\n",
      "\n",
      "Epoch [34/100], Step [101/937], Loss: 0.1742 \n",
      "\n",
      "\n",
      "Epoch [34/100], Step [201/937], Loss: 0.0677 \n",
      "\n",
      "\n",
      "Epoch [34/100], Step [301/937], Loss: 0.0824 \n",
      "\n",
      "\n",
      "Epoch [34/100], Step [401/937], Loss: 0.1017 \n",
      "\n",
      "\n",
      "Epoch [34/100], Step [501/937], Loss: 0.0763 \n",
      "\n",
      "\n",
      "Epoch [34/100], Step [601/937], Loss: 0.1447 \n",
      "\n",
      "\n",
      "Epoch [34/100], Step [701/937], Loss: 0.0464 \n",
      "\n",
      "\n",
      "Epoch [34/100], Step [801/937], Loss: 0.1310 \n",
      "\n",
      "\n",
      "Epoch [34/100], Step [901/937], Loss: 0.0523 \n",
      "\n",
      "\n",
      "Epoch [35/100], Step [1/937], Loss: 0.1369 \n",
      "\n",
      "\n",
      "Epoch [35/100], Step [101/937], Loss: 0.0332 \n",
      "\n",
      "\n",
      "Epoch [35/100], Step [201/937], Loss: 0.0561 \n",
      "\n",
      "\n",
      "Epoch [35/100], Step [301/937], Loss: 0.0948 \n",
      "\n",
      "\n",
      "Epoch [35/100], Step [401/937], Loss: 0.0245 \n",
      "\n",
      "\n",
      "Epoch [35/100], Step [501/937], Loss: 0.0453 \n",
      "\n",
      "\n",
      "Epoch [35/100], Step [601/937], Loss: 0.1068 \n",
      "\n",
      "\n",
      "Epoch [35/100], Step [701/937], Loss: 0.0916 \n",
      "\n",
      "\n",
      "Epoch [35/100], Step [801/937], Loss: 0.3453 \n",
      "\n",
      "\n",
      "Epoch [35/100], Step [901/937], Loss: 0.1787 \n",
      "\n",
      "\n",
      "Epoch [36/100], Step [1/937], Loss: 0.0747 \n",
      "\n",
      "\n",
      "Epoch [36/100], Step [101/937], Loss: 0.0823 \n",
      "\n",
      "\n",
      "Epoch [36/100], Step [201/937], Loss: 0.1655 \n",
      "\n",
      "\n",
      "Epoch [36/100], Step [301/937], Loss: 0.0695 \n",
      "\n",
      "\n",
      "Epoch [36/100], Step [401/937], Loss: 0.1494 \n",
      "\n",
      "\n",
      "Epoch [36/100], Step [501/937], Loss: 0.1467 \n",
      "\n",
      "\n",
      "Epoch [36/100], Step [601/937], Loss: 0.0736 \n",
      "\n",
      "\n",
      "Epoch [36/100], Step [701/937], Loss: 0.0210 \n",
      "\n",
      "\n",
      "Epoch [36/100], Step [801/937], Loss: 0.1545 \n",
      "\n",
      "\n",
      "Epoch [36/100], Step [901/937], Loss: 0.0819 \n",
      "\n",
      "\n",
      "Epoch [37/100], Step [1/937], Loss: 0.1555 \n",
      "\n",
      "\n",
      "Epoch [37/100], Step [101/937], Loss: 0.0902 \n",
      "\n",
      "\n",
      "Epoch [37/100], Step [201/937], Loss: 0.0876 \n",
      "\n",
      "\n",
      "Epoch [37/100], Step [301/937], Loss: 0.0227 \n",
      "\n",
      "\n",
      "Epoch [37/100], Step [401/937], Loss: 0.0481 \n",
      "\n",
      "\n",
      "Epoch [37/100], Step [501/937], Loss: 0.0492 \n",
      "\n",
      "\n",
      "Epoch [37/100], Step [601/937], Loss: 0.0227 \n",
      "\n",
      "\n",
      "Epoch [37/100], Step [701/937], Loss: 0.0780 \n",
      "\n",
      "\n",
      "Epoch [37/100], Step [801/937], Loss: 0.0843 \n",
      "\n",
      "\n",
      "Epoch [37/100], Step [901/937], Loss: 0.2217 \n",
      "\n",
      "\n",
      "Epoch [38/100], Step [1/937], Loss: 0.0697 \n",
      "\n",
      "\n",
      "Epoch [38/100], Step [101/937], Loss: 0.1782 \n",
      "\n",
      "\n",
      "Epoch [38/100], Step [201/937], Loss: 0.1991 \n",
      "\n",
      "\n",
      "Epoch [38/100], Step [301/937], Loss: 0.0858 \n",
      "\n",
      "\n",
      "Epoch [38/100], Step [401/937], Loss: 0.0696 \n",
      "\n",
      "\n",
      "Epoch [38/100], Step [501/937], Loss: 0.1445 \n",
      "\n",
      "\n",
      "Epoch [38/100], Step [601/937], Loss: 0.0266 \n",
      "\n",
      "\n",
      "Epoch [38/100], Step [701/937], Loss: 0.0338 \n",
      "\n",
      "\n",
      "Epoch [38/100], Step [801/937], Loss: 0.0273 \n",
      "\n",
      "\n",
      "Epoch [38/100], Step [901/937], Loss: 0.0294 \n",
      "\n",
      "\n",
      "Epoch [39/100], Step [1/937], Loss: 0.0405 \n",
      "\n",
      "\n",
      "Epoch [39/100], Step [101/937], Loss: 0.0953 \n",
      "\n",
      "\n",
      "Epoch [39/100], Step [201/937], Loss: 0.1051 \n",
      "\n",
      "\n",
      "Epoch [39/100], Step [301/937], Loss: 0.0716 \n",
      "\n",
      "\n",
      "Epoch [39/100], Step [401/937], Loss: 0.1603 \n",
      "\n",
      "\n",
      "Epoch [39/100], Step [501/937], Loss: 0.1421 \n",
      "\n",
      "\n",
      "Epoch [39/100], Step [601/937], Loss: 0.0213 \n",
      "\n",
      "\n",
      "Epoch [39/100], Step [701/937], Loss: 0.0563 \n",
      "\n",
      "\n",
      "Epoch [39/100], Step [801/937], Loss: 0.1637 \n",
      "\n",
      "\n",
      "Epoch [39/100], Step [901/937], Loss: 0.1144 \n",
      "\n",
      "\n",
      "Epoch [40/100], Step [1/937], Loss: 0.1298 \n",
      "\n",
      "\n",
      "Epoch [40/100], Step [101/937], Loss: 0.0648 \n",
      "\n",
      "\n",
      "Epoch [40/100], Step [201/937], Loss: 0.0826 \n",
      "\n",
      "\n",
      "Epoch [40/100], Step [301/937], Loss: 0.1874 \n",
      "\n",
      "\n",
      "Epoch [40/100], Step [401/937], Loss: 0.1889 \n",
      "\n",
      "\n",
      "Epoch [40/100], Step [501/937], Loss: 0.0351 \n",
      "\n",
      "\n",
      "Epoch [40/100], Step [601/937], Loss: 0.0454 \n",
      "\n",
      "\n",
      "Epoch [40/100], Step [701/937], Loss: 0.1006 \n",
      "\n",
      "\n",
      "Epoch [40/100], Step [801/937], Loss: 0.0238 \n",
      "\n",
      "\n",
      "Epoch [40/100], Step [901/937], Loss: 0.2589 \n",
      "\n",
      "\n",
      "Epoch [41/100], Step [1/937], Loss: 0.0549 \n",
      "\n",
      "\n",
      "Epoch [41/100], Step [101/937], Loss: 0.1250 \n",
      "\n",
      "\n",
      "Epoch [41/100], Step [201/937], Loss: 0.0558 \n",
      "\n",
      "\n",
      "Epoch [41/100], Step [301/937], Loss: 0.0604 \n",
      "\n",
      "\n",
      "Epoch [41/100], Step [401/937], Loss: 0.0216 \n",
      "\n",
      "\n",
      "Epoch [41/100], Step [501/937], Loss: 0.0437 \n",
      "\n",
      "\n",
      "Epoch [41/100], Step [601/937], Loss: 0.2047 \n",
      "\n",
      "\n",
      "Epoch [41/100], Step [701/937], Loss: 0.0526 \n",
      "\n",
      "\n",
      "Epoch [41/100], Step [801/937], Loss: 0.0678 \n",
      "\n",
      "\n",
      "Epoch [41/100], Step [901/937], Loss: 0.1113 \n",
      "\n",
      "\n",
      "Epoch [42/100], Step [1/937], Loss: 0.0736 \n",
      "\n",
      "\n",
      "Epoch [42/100], Step [101/937], Loss: 0.0720 \n",
      "\n",
      "\n",
      "Epoch [42/100], Step [201/937], Loss: 0.1839 \n",
      "\n",
      "\n",
      "Epoch [42/100], Step [301/937], Loss: 0.0357 \n",
      "\n",
      "\n",
      "Epoch [42/100], Step [401/937], Loss: 0.0190 \n",
      "\n",
      "\n",
      "Epoch [42/100], Step [501/937], Loss: 0.0348 \n",
      "\n",
      "\n",
      "Epoch [42/100], Step [601/937], Loss: 0.0261 \n",
      "\n",
      "\n",
      "Epoch [42/100], Step [701/937], Loss: 0.1252 \n",
      "\n",
      "\n",
      "Epoch [42/100], Step [801/937], Loss: 0.1823 \n",
      "\n",
      "\n",
      "Epoch [42/100], Step [901/937], Loss: 0.0789 \n",
      "\n",
      "\n",
      "Epoch [43/100], Step [1/937], Loss: 0.1615 \n",
      "\n",
      "\n",
      "Epoch [43/100], Step [101/937], Loss: 0.1041 \n",
      "\n",
      "\n",
      "Epoch [43/100], Step [201/937], Loss: 0.1250 \n",
      "\n",
      "\n",
      "Epoch [43/100], Step [301/937], Loss: 0.2257 \n",
      "\n",
      "\n",
      "Epoch [43/100], Step [401/937], Loss: 0.1088 \n",
      "\n",
      "\n",
      "Epoch [43/100], Step [501/937], Loss: 0.0133 \n",
      "\n",
      "\n",
      "Epoch [43/100], Step [601/937], Loss: 0.0723 \n",
      "\n",
      "\n",
      "Epoch [43/100], Step [701/937], Loss: 0.2794 \n",
      "\n",
      "\n",
      "Epoch [43/100], Step [801/937], Loss: 0.1171 \n",
      "\n",
      "\n",
      "Epoch [43/100], Step [901/937], Loss: 0.0632 \n",
      "\n",
      "\n",
      "Epoch [44/100], Step [1/937], Loss: 0.0688 \n",
      "\n",
      "\n",
      "Epoch [44/100], Step [101/937], Loss: 0.0092 \n",
      "\n",
      "\n",
      "Epoch [44/100], Step [201/937], Loss: 0.1028 \n",
      "\n",
      "\n",
      "Epoch [44/100], Step [301/937], Loss: 0.0782 \n",
      "\n",
      "\n",
      "Epoch [44/100], Step [401/937], Loss: 0.0378 \n",
      "\n",
      "\n",
      "Epoch [44/100], Step [501/937], Loss: 0.0058 \n",
      "\n",
      "\n",
      "Epoch [44/100], Step [601/937], Loss: 0.0869 \n",
      "\n",
      "\n",
      "Epoch [44/100], Step [701/937], Loss: 0.1013 \n",
      "\n",
      "\n",
      "Epoch [44/100], Step [801/937], Loss: 0.1692 \n",
      "\n",
      "\n",
      "Epoch [44/100], Step [901/937], Loss: 0.1440 \n",
      "\n",
      "\n",
      "Epoch [45/100], Step [1/937], Loss: 0.1088 \n",
      "\n",
      "\n",
      "Epoch [45/100], Step [101/937], Loss: 0.0759 \n",
      "\n",
      "\n",
      "Epoch [45/100], Step [201/937], Loss: 0.1077 \n",
      "\n",
      "\n",
      "Epoch [45/100], Step [301/937], Loss: 0.0592 \n",
      "\n",
      "\n",
      "Epoch [45/100], Step [401/937], Loss: 0.1552 \n",
      "\n",
      "\n",
      "Epoch [45/100], Step [501/937], Loss: 0.0790 \n",
      "\n",
      "\n",
      "Epoch [45/100], Step [601/937], Loss: 0.2218 \n",
      "\n",
      "\n",
      "Epoch [45/100], Step [701/937], Loss: 0.0881 \n",
      "\n",
      "\n",
      "Epoch [45/100], Step [801/937], Loss: 0.0518 \n",
      "\n",
      "\n",
      "Epoch [45/100], Step [901/937], Loss: 0.0600 \n",
      "\n",
      "\n",
      "Epoch [46/100], Step [1/937], Loss: 0.0714 \n",
      "\n",
      "\n",
      "Epoch [46/100], Step [101/937], Loss: 0.1080 \n",
      "\n",
      "\n",
      "Epoch [46/100], Step [201/937], Loss: 0.1298 \n",
      "\n",
      "\n",
      "Epoch [46/100], Step [301/937], Loss: 0.1025 \n",
      "\n",
      "\n",
      "Epoch [46/100], Step [401/937], Loss: 0.1783 \n",
      "\n",
      "\n",
      "Epoch [46/100], Step [501/937], Loss: 0.0181 \n",
      "\n",
      "\n",
      "Epoch [46/100], Step [601/937], Loss: 0.0883 \n",
      "\n",
      "\n",
      "Epoch [46/100], Step [701/937], Loss: 0.0584 \n",
      "\n",
      "\n",
      "Epoch [46/100], Step [801/937], Loss: 0.0211 \n",
      "\n",
      "\n",
      "Epoch [46/100], Step [901/937], Loss: 0.1635 \n",
      "\n",
      "\n",
      "Epoch [47/100], Step [1/937], Loss: 0.0854 \n",
      "\n",
      "\n",
      "Epoch [47/100], Step [101/937], Loss: 0.0159 \n",
      "\n",
      "\n",
      "Epoch [47/100], Step [201/937], Loss: 0.1123 \n",
      "\n",
      "\n",
      "Epoch [47/100], Step [301/937], Loss: 0.0879 \n",
      "\n",
      "\n",
      "Epoch [47/100], Step [401/937], Loss: 0.1912 \n",
      "\n",
      "\n",
      "Epoch [47/100], Step [501/937], Loss: 0.1555 \n",
      "\n",
      "\n",
      "Epoch [47/100], Step [601/937], Loss: 0.1234 \n",
      "\n",
      "\n",
      "Epoch [47/100], Step [701/937], Loss: 0.0584 \n",
      "\n",
      "\n",
      "Epoch [47/100], Step [801/937], Loss: 0.0957 \n",
      "\n",
      "\n",
      "Epoch [47/100], Step [901/937], Loss: 0.1992 \n",
      "\n",
      "\n",
      "Epoch [48/100], Step [1/937], Loss: 0.0648 \n",
      "\n",
      "\n",
      "Epoch [48/100], Step [101/937], Loss: 0.1038 \n",
      "\n",
      "\n",
      "Epoch [48/100], Step [201/937], Loss: 0.1037 \n",
      "\n",
      "\n",
      "Epoch [48/100], Step [301/937], Loss: 0.1080 \n",
      "\n",
      "\n",
      "Epoch [48/100], Step [401/937], Loss: 0.0127 \n",
      "\n",
      "\n",
      "Epoch [48/100], Step [501/937], Loss: 0.0930 \n",
      "\n",
      "\n",
      "Epoch [48/100], Step [601/937], Loss: 0.0992 \n",
      "\n",
      "\n",
      "Epoch [48/100], Step [701/937], Loss: 0.1360 \n",
      "\n",
      "\n",
      "Epoch [48/100], Step [801/937], Loss: 0.1526 \n",
      "\n",
      "\n",
      "Epoch [48/100], Step [901/937], Loss: 0.1521 \n",
      "\n",
      "\n",
      "Epoch [49/100], Step [1/937], Loss: 0.1403 \n",
      "\n",
      "\n",
      "Epoch [49/100], Step [101/937], Loss: 0.1904 \n",
      "\n",
      "\n",
      "Epoch [49/100], Step [201/937], Loss: 0.0337 \n",
      "\n",
      "\n",
      "Epoch [49/100], Step [301/937], Loss: 0.1589 \n",
      "\n",
      "\n",
      "Epoch [49/100], Step [401/937], Loss: 0.0517 \n",
      "\n",
      "\n",
      "Epoch [49/100], Step [501/937], Loss: 0.0198 \n",
      "\n",
      "\n",
      "Epoch [49/100], Step [601/937], Loss: 0.0689 \n",
      "\n",
      "\n",
      "Epoch [49/100], Step [701/937], Loss: 0.0758 \n",
      "\n",
      "\n",
      "Epoch [49/100], Step [801/937], Loss: 0.2184 \n",
      "\n",
      "\n",
      "Epoch [49/100], Step [901/937], Loss: 0.0829 \n",
      "\n",
      "\n",
      "Epoch [50/100], Step [1/937], Loss: 0.0697 \n",
      "\n",
      "\n",
      "Epoch [50/100], Step [101/937], Loss: 0.0963 \n",
      "\n",
      "\n",
      "Epoch [50/100], Step [201/937], Loss: 0.1857 \n",
      "\n",
      "\n",
      "Epoch [50/100], Step [301/937], Loss: 0.0432 \n",
      "\n",
      "\n",
      "Epoch [50/100], Step [401/937], Loss: 0.1666 \n",
      "\n",
      "\n",
      "Epoch [50/100], Step [501/937], Loss: 0.0175 \n",
      "\n",
      "\n",
      "Epoch [50/100], Step [601/937], Loss: 0.0466 \n",
      "\n",
      "\n",
      "Epoch [50/100], Step [701/937], Loss: 0.1572 \n",
      "\n",
      "\n",
      "Epoch [50/100], Step [801/937], Loss: 0.2547 \n",
      "\n",
      "\n",
      "Epoch [50/100], Step [901/937], Loss: 0.1081 \n",
      "\n",
      "\n",
      "Epoch [51/100], Step [1/937], Loss: 0.1130 \n",
      "\n",
      "\n",
      "Epoch [51/100], Step [101/937], Loss: 0.1513 \n",
      "\n",
      "\n",
      "Epoch [51/100], Step [201/937], Loss: 0.0096 \n",
      "\n",
      "\n",
      "Epoch [51/100], Step [301/937], Loss: 0.1335 \n",
      "\n",
      "\n",
      "Epoch [51/100], Step [401/937], Loss: 0.1055 \n",
      "\n",
      "\n",
      "Epoch [51/100], Step [501/937], Loss: 0.0585 \n",
      "\n",
      "\n",
      "Epoch [51/100], Step [601/937], Loss: 0.1318 \n",
      "\n",
      "\n",
      "Epoch [51/100], Step [701/937], Loss: 0.0174 \n",
      "\n",
      "\n",
      "Epoch [51/100], Step [801/937], Loss: 0.0884 \n",
      "\n",
      "\n",
      "Epoch [51/100], Step [901/937], Loss: 0.0660 \n",
      "\n",
      "\n",
      "Epoch [52/100], Step [1/937], Loss: 0.0345 \n",
      "\n",
      "\n",
      "Epoch [52/100], Step [101/937], Loss: 0.0741 \n",
      "\n",
      "\n",
      "Epoch [52/100], Step [201/937], Loss: 0.0427 \n",
      "\n",
      "\n",
      "Epoch [52/100], Step [301/937], Loss: 0.1074 \n",
      "\n",
      "\n",
      "Epoch [52/100], Step [401/937], Loss: 0.1463 \n",
      "\n",
      "\n",
      "Epoch [52/100], Step [501/937], Loss: 0.0383 \n",
      "\n",
      "\n",
      "Epoch [52/100], Step [601/937], Loss: 0.0109 \n",
      "\n",
      "\n",
      "Epoch [52/100], Step [701/937], Loss: 0.0324 \n",
      "\n",
      "\n",
      "Epoch [52/100], Step [801/937], Loss: 0.0326 \n",
      "\n",
      "\n",
      "Epoch [52/100], Step [901/937], Loss: 0.0278 \n",
      "\n",
      "\n",
      "Epoch [53/100], Step [1/937], Loss: 0.0264 \n",
      "\n",
      "\n",
      "Epoch [53/100], Step [101/937], Loss: 0.0757 \n",
      "\n",
      "\n",
      "Epoch [53/100], Step [201/937], Loss: 0.0803 \n",
      "\n",
      "\n",
      "Epoch [53/100], Step [301/937], Loss: 0.1029 \n",
      "\n",
      "\n",
      "Epoch [53/100], Step [401/937], Loss: 0.0548 \n",
      "\n",
      "\n",
      "Epoch [53/100], Step [501/937], Loss: 0.0281 \n",
      "\n",
      "\n",
      "Epoch [53/100], Step [601/937], Loss: 0.0707 \n",
      "\n",
      "\n",
      "Epoch [53/100], Step [701/937], Loss: 0.1050 \n",
      "\n",
      "\n",
      "Epoch [53/100], Step [801/937], Loss: 0.1012 \n",
      "\n",
      "\n",
      "Epoch [53/100], Step [901/937], Loss: 0.0568 \n",
      "\n",
      "\n",
      "Epoch [54/100], Step [1/937], Loss: 0.0707 \n",
      "\n",
      "\n",
      "Epoch [54/100], Step [101/937], Loss: 0.1061 \n",
      "\n",
      "\n",
      "Epoch [54/100], Step [201/937], Loss: 0.0536 \n",
      "\n",
      "\n",
      "Epoch [54/100], Step [301/937], Loss: 0.0634 \n",
      "\n",
      "\n",
      "Epoch [54/100], Step [401/937], Loss: 0.1263 \n",
      "\n",
      "\n",
      "Epoch [54/100], Step [501/937], Loss: 0.0365 \n",
      "\n",
      "\n",
      "Epoch [54/100], Step [601/937], Loss: 0.0535 \n",
      "\n",
      "\n",
      "Epoch [54/100], Step [701/937], Loss: 0.1101 \n",
      "\n",
      "\n",
      "Epoch [54/100], Step [801/937], Loss: 0.0517 \n",
      "\n",
      "\n",
      "Epoch [54/100], Step [901/937], Loss: 0.0221 \n",
      "\n",
      "\n",
      "Epoch [55/100], Step [1/937], Loss: 0.0572 \n",
      "\n",
      "\n",
      "Epoch [55/100], Step [101/937], Loss: 0.0695 \n",
      "\n",
      "\n",
      "Epoch [55/100], Step [201/937], Loss: 0.1244 \n",
      "\n",
      "\n",
      "Epoch [55/100], Step [301/937], Loss: 0.0349 \n",
      "\n",
      "\n",
      "Epoch [55/100], Step [401/937], Loss: 0.0606 \n",
      "\n",
      "\n",
      "Epoch [55/100], Step [501/937], Loss: 0.0506 \n",
      "\n",
      "\n",
      "Epoch [55/100], Step [601/937], Loss: 0.0614 \n",
      "\n",
      "\n",
      "Epoch [55/100], Step [701/937], Loss: 0.0897 \n",
      "\n",
      "\n",
      "Epoch [55/100], Step [801/937], Loss: 0.1119 \n",
      "\n",
      "\n",
      "Epoch [55/100], Step [901/937], Loss: 0.0433 \n",
      "\n",
      "\n",
      "Epoch [56/100], Step [1/937], Loss: 0.0835 \n",
      "\n",
      "\n",
      "Epoch [56/100], Step [101/937], Loss: 0.0349 \n",
      "\n",
      "\n",
      "Epoch [56/100], Step [201/937], Loss: 0.0670 \n",
      "\n",
      "\n",
      "Epoch [56/100], Step [301/937], Loss: 0.1418 \n",
      "\n",
      "\n",
      "Epoch [56/100], Step [401/937], Loss: 0.1829 \n",
      "\n",
      "\n",
      "Epoch [56/100], Step [501/937], Loss: 0.0675 \n",
      "\n",
      "\n",
      "Epoch [56/100], Step [601/937], Loss: 0.0581 \n",
      "\n",
      "\n",
      "Epoch [56/100], Step [701/937], Loss: 0.0735 \n",
      "\n",
      "\n",
      "Epoch [56/100], Step [801/937], Loss: 0.0504 \n",
      "\n",
      "\n",
      "Epoch [56/100], Step [901/937], Loss: 0.0684 \n",
      "\n",
      "\n",
      "Epoch [57/100], Step [1/937], Loss: 0.0969 \n",
      "\n",
      "\n",
      "Epoch [57/100], Step [101/937], Loss: 0.0209 \n",
      "\n",
      "\n",
      "Epoch [57/100], Step [201/937], Loss: 0.1033 \n",
      "\n",
      "\n",
      "Epoch [57/100], Step [301/937], Loss: 0.0446 \n",
      "\n",
      "\n",
      "Epoch [57/100], Step [401/937], Loss: 0.0661 \n",
      "\n",
      "\n",
      "Epoch [57/100], Step [501/937], Loss: 0.1317 \n",
      "\n",
      "\n",
      "Epoch [57/100], Step [601/937], Loss: 0.0268 \n",
      "\n",
      "\n",
      "Epoch [57/100], Step [701/937], Loss: 0.0422 \n",
      "\n",
      "\n",
      "Epoch [57/100], Step [801/937], Loss: 0.0242 \n",
      "\n",
      "\n",
      "Epoch [57/100], Step [901/937], Loss: 0.0808 \n",
      "\n",
      "\n",
      "Epoch [58/100], Step [1/937], Loss: 0.0625 \n",
      "\n",
      "\n",
      "Epoch [58/100], Step [101/937], Loss: 0.0339 \n",
      "\n",
      "\n",
      "Epoch [58/100], Step [201/937], Loss: 0.2172 \n",
      "\n",
      "\n",
      "Epoch [58/100], Step [301/937], Loss: 0.1045 \n",
      "\n",
      "\n",
      "Epoch [58/100], Step [401/937], Loss: 0.1602 \n",
      "\n",
      "\n",
      "Epoch [58/100], Step [501/937], Loss: 0.0740 \n",
      "\n",
      "\n",
      "Epoch [58/100], Step [601/937], Loss: 0.0640 \n",
      "\n",
      "\n",
      "Epoch [58/100], Step [701/937], Loss: 0.1230 \n",
      "\n",
      "\n",
      "Epoch [58/100], Step [801/937], Loss: 0.1254 \n",
      "\n",
      "\n",
      "Epoch [58/100], Step [901/937], Loss: 0.2183 \n",
      "\n",
      "\n",
      "Epoch [59/100], Step [1/937], Loss: 0.0368 \n",
      "\n",
      "\n",
      "Epoch [59/100], Step [101/937], Loss: 0.0919 \n",
      "\n",
      "\n",
      "Epoch [59/100], Step [201/937], Loss: 0.0261 \n",
      "\n",
      "\n",
      "Epoch [59/100], Step [301/937], Loss: 0.0765 \n",
      "\n",
      "\n",
      "Epoch [59/100], Step [401/937], Loss: 0.0727 \n",
      "\n",
      "\n",
      "Epoch [59/100], Step [501/937], Loss: 0.2598 \n",
      "\n",
      "\n",
      "Epoch [59/100], Step [601/937], Loss: 0.0363 \n",
      "\n",
      "\n",
      "Epoch [59/100], Step [701/937], Loss: 0.0349 \n",
      "\n",
      "\n",
      "Epoch [59/100], Step [801/937], Loss: 0.1455 \n",
      "\n",
      "\n",
      "Epoch [59/100], Step [901/937], Loss: 0.1806 \n",
      "\n",
      "\n",
      "Epoch [60/100], Step [1/937], Loss: 0.0556 \n",
      "\n",
      "\n",
      "Epoch [60/100], Step [101/937], Loss: 0.1037 \n",
      "\n",
      "\n",
      "Epoch [60/100], Step [201/937], Loss: 0.0908 \n",
      "\n",
      "\n",
      "Epoch [60/100], Step [301/937], Loss: 0.0431 \n",
      "\n",
      "\n",
      "Epoch [60/100], Step [401/937], Loss: 0.0350 \n",
      "\n",
      "\n",
      "Epoch [60/100], Step [501/937], Loss: 0.0454 \n",
      "\n",
      "\n",
      "Epoch [60/100], Step [601/937], Loss: 0.0673 \n",
      "\n",
      "\n",
      "Epoch [60/100], Step [701/937], Loss: 0.0379 \n",
      "\n",
      "\n",
      "Epoch [60/100], Step [801/937], Loss: 0.0722 \n",
      "\n",
      "\n",
      "Epoch [60/100], Step [901/937], Loss: 0.0480 \n",
      "\n",
      "\n",
      "Epoch [61/100], Step [1/937], Loss: 0.1134 \n",
      "\n",
      "\n",
      "Epoch [61/100], Step [101/937], Loss: 0.1730 \n",
      "\n",
      "\n",
      "Epoch [61/100], Step [201/937], Loss: 0.2797 \n",
      "\n",
      "\n",
      "Epoch [61/100], Step [301/937], Loss: 0.0372 \n",
      "\n",
      "\n",
      "Epoch [61/100], Step [401/937], Loss: 0.2286 \n",
      "\n",
      "\n",
      "Epoch [61/100], Step [501/937], Loss: 0.1070 \n",
      "\n",
      "\n",
      "Epoch [61/100], Step [601/937], Loss: 0.0231 \n",
      "\n",
      "\n",
      "Epoch [61/100], Step [701/937], Loss: 0.0611 \n",
      "\n",
      "\n",
      "Epoch [61/100], Step [801/937], Loss: 0.0810 \n",
      "\n",
      "\n",
      "Epoch [61/100], Step [901/937], Loss: 0.1003 \n",
      "\n",
      "\n",
      "Epoch [62/100], Step [1/937], Loss: 0.0424 \n",
      "\n",
      "\n",
      "Epoch [62/100], Step [101/937], Loss: 0.2136 \n",
      "\n",
      "\n",
      "Epoch [62/100], Step [201/937], Loss: 0.0764 \n",
      "\n",
      "\n",
      "Epoch [62/100], Step [301/937], Loss: 0.0228 \n",
      "\n",
      "\n",
      "Epoch [62/100], Step [401/937], Loss: 0.0205 \n",
      "\n",
      "\n",
      "Epoch [62/100], Step [501/937], Loss: 0.0620 \n",
      "\n",
      "\n",
      "Epoch [62/100], Step [601/937], Loss: 0.0773 \n",
      "\n",
      "\n",
      "Epoch [62/100], Step [701/937], Loss: 0.0454 \n",
      "\n",
      "\n",
      "Epoch [62/100], Step [801/937], Loss: 0.1545 \n",
      "\n",
      "\n",
      "Epoch [62/100], Step [901/937], Loss: 0.0910 \n",
      "\n",
      "\n",
      "Epoch [63/100], Step [1/937], Loss: 0.0593 \n",
      "\n",
      "\n",
      "Epoch [63/100], Step [101/937], Loss: 0.1010 \n",
      "\n",
      "\n",
      "Epoch [63/100], Step [201/937], Loss: 0.0724 \n",
      "\n",
      "\n",
      "Epoch [63/100], Step [301/937], Loss: 0.1549 \n",
      "\n",
      "\n",
      "Epoch [63/100], Step [401/937], Loss: 0.0476 \n",
      "\n",
      "\n",
      "Epoch [63/100], Step [501/937], Loss: 0.1016 \n",
      "\n",
      "\n",
      "Epoch [63/100], Step [601/937], Loss: 0.0510 \n",
      "\n",
      "\n",
      "Epoch [63/100], Step [701/937], Loss: 0.0750 \n",
      "\n",
      "\n",
      "Epoch [63/100], Step [801/937], Loss: 0.0337 \n",
      "\n",
      "\n",
      "Epoch [63/100], Step [901/937], Loss: 0.1378 \n",
      "\n",
      "\n",
      "Epoch [64/100], Step [1/937], Loss: 0.0431 \n",
      "\n",
      "\n",
      "Epoch [64/100], Step [101/937], Loss: 0.0882 \n",
      "\n",
      "\n",
      "Epoch [64/100], Step [201/937], Loss: 0.0617 \n",
      "\n",
      "\n",
      "Epoch [64/100], Step [301/937], Loss: 0.1033 \n",
      "\n",
      "\n",
      "Epoch [64/100], Step [401/937], Loss: 0.1359 \n",
      "\n",
      "\n",
      "Epoch [64/100], Step [501/937], Loss: 0.1006 \n",
      "\n",
      "\n",
      "Epoch [64/100], Step [601/937], Loss: 0.0346 \n",
      "\n",
      "\n",
      "Epoch [64/100], Step [701/937], Loss: 0.0968 \n",
      "\n",
      "\n",
      "Epoch [64/100], Step [801/937], Loss: 0.0810 \n",
      "\n",
      "\n",
      "Epoch [64/100], Step [901/937], Loss: 0.0282 \n",
      "\n",
      "\n",
      "Epoch [65/100], Step [1/937], Loss: 0.0483 \n",
      "\n",
      "\n",
      "Epoch [65/100], Step [101/937], Loss: 0.1542 \n",
      "\n",
      "\n",
      "Epoch [65/100], Step [201/937], Loss: 0.0425 \n",
      "\n",
      "\n",
      "Epoch [65/100], Step [301/937], Loss: 0.1625 \n",
      "\n",
      "\n",
      "Epoch [65/100], Step [401/937], Loss: 0.0342 \n",
      "\n",
      "\n",
      "Epoch [65/100], Step [501/937], Loss: 0.0988 \n",
      "\n",
      "\n",
      "Epoch [65/100], Step [601/937], Loss: 0.0266 \n",
      "\n",
      "\n",
      "Epoch [65/100], Step [701/937], Loss: 0.0830 \n",
      "\n",
      "\n",
      "Epoch [65/100], Step [801/937], Loss: 0.0410 \n",
      "\n",
      "\n",
      "Epoch [65/100], Step [901/937], Loss: 0.0222 \n",
      "\n",
      "\n",
      "Epoch [66/100], Step [1/937], Loss: 0.1034 \n",
      "\n",
      "\n",
      "Epoch [66/100], Step [101/937], Loss: 0.0819 \n",
      "\n",
      "\n",
      "Epoch [66/100], Step [201/937], Loss: 0.1274 \n",
      "\n",
      "\n",
      "Epoch [66/100], Step [301/937], Loss: 0.0497 \n",
      "\n",
      "\n",
      "Epoch [66/100], Step [401/937], Loss: 0.1579 \n",
      "\n",
      "\n",
      "Epoch [66/100], Step [501/937], Loss: 0.0564 \n",
      "\n",
      "\n",
      "Epoch [66/100], Step [601/937], Loss: 0.1179 \n",
      "\n",
      "\n",
      "Epoch [66/100], Step [701/937], Loss: 0.0433 \n",
      "\n",
      "\n",
      "Epoch [66/100], Step [801/937], Loss: 0.1497 \n",
      "\n",
      "\n",
      "Epoch [66/100], Step [901/937], Loss: 0.0982 \n",
      "\n",
      "\n",
      "Epoch [67/100], Step [1/937], Loss: 0.0462 \n",
      "\n",
      "\n",
      "Epoch [67/100], Step [101/937], Loss: 0.0235 \n",
      "\n",
      "\n",
      "Epoch [67/100], Step [201/937], Loss: 0.0212 \n",
      "\n",
      "\n",
      "Epoch [67/100], Step [301/937], Loss: 0.0838 \n",
      "\n",
      "\n",
      "Epoch [67/100], Step [401/937], Loss: 0.0387 \n",
      "\n",
      "\n",
      "Epoch [67/100], Step [501/937], Loss: 0.1569 \n",
      "\n",
      "\n",
      "Epoch [67/100], Step [601/937], Loss: 0.0421 \n",
      "\n",
      "\n",
      "Epoch [67/100], Step [701/937], Loss: 0.0136 \n",
      "\n",
      "\n",
      "Epoch [67/100], Step [801/937], Loss: 0.1204 \n",
      "\n",
      "\n",
      "Epoch [67/100], Step [901/937], Loss: 0.0953 \n",
      "\n",
      "\n",
      "Epoch [68/100], Step [1/937], Loss: 0.1325 \n",
      "\n",
      "\n",
      "Epoch [68/100], Step [101/937], Loss: 0.0621 \n",
      "\n",
      "\n",
      "Epoch [68/100], Step [201/937], Loss: 0.1021 \n",
      "\n",
      "\n",
      "Epoch [68/100], Step [301/937], Loss: 0.0629 \n",
      "\n",
      "\n",
      "Epoch [68/100], Step [401/937], Loss: 0.0550 \n",
      "\n",
      "\n",
      "Epoch [68/100], Step [501/937], Loss: 0.1942 \n",
      "\n",
      "\n",
      "Epoch [68/100], Step [601/937], Loss: 0.0409 \n",
      "\n",
      "\n",
      "Epoch [68/100], Step [701/937], Loss: 0.0172 \n",
      "\n",
      "\n",
      "Epoch [68/100], Step [801/937], Loss: 0.0927 \n",
      "\n",
      "\n",
      "Epoch [68/100], Step [901/937], Loss: 0.0775 \n",
      "\n",
      "\n",
      "Epoch [69/100], Step [1/937], Loss: 0.0250 \n",
      "\n",
      "\n",
      "Epoch [69/100], Step [101/937], Loss: 0.0862 \n",
      "\n",
      "\n",
      "Epoch [69/100], Step [201/937], Loss: 0.1028 \n",
      "\n",
      "\n",
      "Epoch [69/100], Step [301/937], Loss: 0.0196 \n",
      "\n",
      "\n",
      "Epoch [69/100], Step [401/937], Loss: 0.1502 \n",
      "\n",
      "\n",
      "Epoch [69/100], Step [501/937], Loss: 0.0923 \n",
      "\n",
      "\n",
      "Epoch [69/100], Step [601/937], Loss: 0.0813 \n",
      "\n",
      "\n",
      "Epoch [69/100], Step [701/937], Loss: 0.0678 \n",
      "\n",
      "\n",
      "Epoch [69/100], Step [801/937], Loss: 0.0952 \n",
      "\n",
      "\n",
      "Epoch [69/100], Step [901/937], Loss: 0.0259 \n",
      "\n",
      "\n",
      "Epoch [70/100], Step [1/937], Loss: 0.0791 \n",
      "\n",
      "\n",
      "Epoch [70/100], Step [101/937], Loss: 0.0938 \n",
      "\n",
      "\n",
      "Epoch [70/100], Step [201/937], Loss: 0.0324 \n",
      "\n",
      "\n",
      "Epoch [70/100], Step [301/937], Loss: 0.1013 \n",
      "\n",
      "\n",
      "Epoch [70/100], Step [401/937], Loss: 0.0830 \n",
      "\n",
      "\n",
      "Epoch [70/100], Step [501/937], Loss: 0.0675 \n",
      "\n",
      "\n",
      "Epoch [70/100], Step [601/937], Loss: 0.0108 \n",
      "\n",
      "\n",
      "Epoch [70/100], Step [701/937], Loss: 0.0109 \n",
      "\n",
      "\n",
      "Epoch [70/100], Step [801/937], Loss: 0.0808 \n",
      "\n",
      "\n",
      "Epoch [70/100], Step [901/937], Loss: 0.1698 \n",
      "\n",
      "\n",
      "Epoch [71/100], Step [1/937], Loss: 0.0690 \n",
      "\n",
      "\n",
      "Epoch [71/100], Step [101/937], Loss: 0.1066 \n",
      "\n",
      "\n",
      "Epoch [71/100], Step [201/937], Loss: 0.0668 \n",
      "\n",
      "\n",
      "Epoch [71/100], Step [301/937], Loss: 0.0236 \n",
      "\n",
      "\n",
      "Epoch [71/100], Step [401/937], Loss: 0.0121 \n",
      "\n",
      "\n",
      "Epoch [71/100], Step [501/937], Loss: 0.0210 \n",
      "\n",
      "\n",
      "Epoch [71/100], Step [601/937], Loss: 0.0785 \n",
      "\n",
      "\n",
      "Epoch [71/100], Step [701/937], Loss: 0.0902 \n",
      "\n",
      "\n",
      "Epoch [71/100], Step [801/937], Loss: 0.1871 \n",
      "\n",
      "\n",
      "Epoch [71/100], Step [901/937], Loss: 0.0513 \n",
      "\n",
      "\n",
      "Epoch [72/100], Step [1/937], Loss: 0.0158 \n",
      "\n",
      "\n",
      "Epoch [72/100], Step [101/937], Loss: 0.1010 \n",
      "\n",
      "\n",
      "Epoch [72/100], Step [201/937], Loss: 0.0380 \n",
      "\n",
      "\n",
      "Epoch [72/100], Step [301/937], Loss: 0.0313 \n",
      "\n",
      "\n",
      "Epoch [72/100], Step [401/937], Loss: 0.0469 \n",
      "\n",
      "\n",
      "Epoch [72/100], Step [501/937], Loss: 0.1260 \n",
      "\n",
      "\n",
      "Epoch [72/100], Step [601/937], Loss: 0.0684 \n",
      "\n",
      "\n",
      "Epoch [72/100], Step [701/937], Loss: 0.2131 \n",
      "\n",
      "\n",
      "Epoch [72/100], Step [801/937], Loss: 0.0304 \n",
      "\n",
      "\n",
      "Epoch [72/100], Step [901/937], Loss: 0.0689 \n",
      "\n",
      "\n",
      "Epoch [73/100], Step [1/937], Loss: 0.0326 \n",
      "\n",
      "\n",
      "Epoch [73/100], Step [101/937], Loss: 0.0219 \n",
      "\n",
      "\n",
      "Epoch [73/100], Step [201/937], Loss: 0.0268 \n",
      "\n",
      "\n",
      "Epoch [73/100], Step [301/937], Loss: 0.0148 \n",
      "\n",
      "\n",
      "Epoch [73/100], Step [401/937], Loss: 0.0550 \n",
      "\n",
      "\n",
      "Epoch [73/100], Step [501/937], Loss: 0.0133 \n",
      "\n",
      "\n",
      "Epoch [73/100], Step [601/937], Loss: 0.0476 \n",
      "\n",
      "\n",
      "Epoch [73/100], Step [701/937], Loss: 0.0842 \n",
      "\n",
      "\n",
      "Epoch [73/100], Step [801/937], Loss: 0.0994 \n",
      "\n",
      "\n",
      "Epoch [73/100], Step [901/937], Loss: 0.1228 \n",
      "\n",
      "\n",
      "Epoch [74/100], Step [1/937], Loss: 0.0734 \n",
      "\n",
      "\n",
      "Epoch [74/100], Step [101/937], Loss: 0.0530 \n",
      "\n",
      "\n",
      "Epoch [74/100], Step [201/937], Loss: 0.0849 \n",
      "\n",
      "\n",
      "Epoch [74/100], Step [301/937], Loss: 0.0600 \n",
      "\n",
      "\n",
      "Epoch [74/100], Step [401/937], Loss: 0.0505 \n",
      "\n",
      "\n",
      "Epoch [74/100], Step [501/937], Loss: 0.0245 \n",
      "\n",
      "\n",
      "Epoch [74/100], Step [601/937], Loss: 0.1585 \n",
      "\n",
      "\n",
      "Epoch [74/100], Step [701/937], Loss: 0.0561 \n",
      "\n",
      "\n",
      "Epoch [74/100], Step [801/937], Loss: 0.0911 \n",
      "\n",
      "\n",
      "Epoch [74/100], Step [901/937], Loss: 0.0303 \n",
      "\n",
      "\n",
      "Epoch [75/100], Step [1/937], Loss: 0.0428 \n",
      "\n",
      "\n",
      "Epoch [75/100], Step [101/937], Loss: 0.0496 \n",
      "\n",
      "\n",
      "Epoch [75/100], Step [201/937], Loss: 0.0693 \n",
      "\n",
      "\n",
      "Epoch [75/100], Step [301/937], Loss: 0.0176 \n",
      "\n",
      "\n",
      "Epoch [75/100], Step [401/937], Loss: 0.0451 \n",
      "\n",
      "\n",
      "Epoch [75/100], Step [501/937], Loss: 0.2082 \n",
      "\n",
      "\n",
      "Epoch [75/100], Step [601/937], Loss: 0.1408 \n",
      "\n",
      "\n",
      "Epoch [75/100], Step [701/937], Loss: 0.0302 \n",
      "\n",
      "\n",
      "Epoch [75/100], Step [801/937], Loss: 0.0641 \n",
      "\n",
      "\n",
      "Epoch [75/100], Step [901/937], Loss: 0.1760 \n",
      "\n",
      "\n",
      "Epoch [76/100], Step [1/937], Loss: 0.0928 \n",
      "\n",
      "\n",
      "Epoch [76/100], Step [101/937], Loss: 0.1685 \n",
      "\n",
      "\n",
      "Epoch [76/100], Step [201/937], Loss: 0.0713 \n",
      "\n",
      "\n",
      "Epoch [76/100], Step [301/937], Loss: 0.0523 \n",
      "\n",
      "\n",
      "Epoch [76/100], Step [401/937], Loss: 0.0349 \n",
      "\n",
      "\n",
      "Epoch [76/100], Step [501/937], Loss: 0.0375 \n",
      "\n",
      "\n",
      "Epoch [76/100], Step [601/937], Loss: 0.1923 \n",
      "\n",
      "\n",
      "Epoch [76/100], Step [701/937], Loss: 0.0973 \n",
      "\n",
      "\n",
      "Epoch [76/100], Step [801/937], Loss: 0.0893 \n",
      "\n",
      "\n",
      "Epoch [76/100], Step [901/937], Loss: 0.3821 \n",
      "\n",
      "\n",
      "Epoch [77/100], Step [1/937], Loss: 0.0915 \n",
      "\n",
      "\n",
      "Epoch [77/100], Step [101/937], Loss: 0.0462 \n",
      "\n",
      "\n",
      "Epoch [77/100], Step [201/937], Loss: 0.0762 \n",
      "\n",
      "\n",
      "Epoch [77/100], Step [301/937], Loss: 0.0418 \n",
      "\n",
      "\n",
      "Epoch [77/100], Step [401/937], Loss: 0.0360 \n",
      "\n",
      "\n",
      "Epoch [77/100], Step [501/937], Loss: 0.0438 \n",
      "\n",
      "\n",
      "Epoch [77/100], Step [601/937], Loss: 0.0480 \n",
      "\n",
      "\n",
      "Epoch [77/100], Step [701/937], Loss: 0.1530 \n",
      "\n",
      "\n",
      "Epoch [77/100], Step [801/937], Loss: 0.0471 \n",
      "\n",
      "\n",
      "Epoch [77/100], Step [901/937], Loss: 0.0282 \n",
      "\n",
      "\n",
      "Epoch [78/100], Step [1/937], Loss: 0.0932 \n",
      "\n",
      "\n",
      "Epoch [78/100], Step [101/937], Loss: 0.0392 \n",
      "\n",
      "\n",
      "Epoch [78/100], Step [201/937], Loss: 0.1492 \n",
      "\n",
      "\n",
      "Epoch [78/100], Step [301/937], Loss: 0.1210 \n",
      "\n",
      "\n",
      "Epoch [78/100], Step [401/937], Loss: 0.1798 \n",
      "\n",
      "\n",
      "Epoch [78/100], Step [501/937], Loss: 0.0627 \n",
      "\n",
      "\n",
      "Epoch [78/100], Step [601/937], Loss: 0.0473 \n",
      "\n",
      "\n",
      "Epoch [78/100], Step [701/937], Loss: 0.1047 \n",
      "\n",
      "\n",
      "Epoch [78/100], Step [801/937], Loss: 0.1278 \n",
      "\n",
      "\n",
      "Epoch [78/100], Step [901/937], Loss: 0.1417 \n",
      "\n",
      "\n",
      "Epoch [79/100], Step [1/937], Loss: 0.0542 \n",
      "\n",
      "\n",
      "Epoch [79/100], Step [101/937], Loss: 0.0742 \n",
      "\n",
      "\n",
      "Epoch [79/100], Step [201/937], Loss: 0.0804 \n",
      "\n",
      "\n",
      "Epoch [79/100], Step [301/937], Loss: 0.0874 \n",
      "\n",
      "\n",
      "Epoch [79/100], Step [401/937], Loss: 0.0242 \n",
      "\n",
      "\n",
      "Epoch [79/100], Step [501/937], Loss: 0.0585 \n",
      "\n",
      "\n",
      "Epoch [79/100], Step [601/937], Loss: 0.0937 \n",
      "\n",
      "\n",
      "Epoch [79/100], Step [701/937], Loss: 0.1521 \n",
      "\n",
      "\n",
      "Epoch [79/100], Step [801/937], Loss: 0.0810 \n",
      "\n",
      "\n",
      "Epoch [79/100], Step [901/937], Loss: 0.1320 \n",
      "\n",
      "\n",
      "Epoch [80/100], Step [1/937], Loss: 0.0314 \n",
      "\n",
      "\n",
      "Epoch [80/100], Step [101/937], Loss: 0.0465 \n",
      "\n",
      "\n",
      "Epoch [80/100], Step [201/937], Loss: 0.0295 \n",
      "\n",
      "\n",
      "Epoch [80/100], Step [301/937], Loss: 0.0520 \n",
      "\n",
      "\n",
      "Epoch [80/100], Step [401/937], Loss: 0.0989 \n",
      "\n",
      "\n",
      "Epoch [80/100], Step [501/937], Loss: 0.0943 \n",
      "\n",
      "\n",
      "Epoch [80/100], Step [601/937], Loss: 0.1115 \n",
      "\n",
      "\n",
      "Epoch [80/100], Step [701/937], Loss: 0.0393 \n",
      "\n",
      "\n",
      "Epoch [80/100], Step [801/937], Loss: 0.0222 \n",
      "\n",
      "\n",
      "Epoch [80/100], Step [901/937], Loss: 0.0923 \n",
      "\n",
      "\n",
      "Epoch [81/100], Step [1/937], Loss: 0.1224 \n",
      "\n",
      "\n",
      "Epoch [81/100], Step [101/937], Loss: 0.0722 \n",
      "\n",
      "\n",
      "Epoch [81/100], Step [201/937], Loss: 0.0536 \n",
      "\n",
      "\n",
      "Epoch [81/100], Step [301/937], Loss: 0.0487 \n",
      "\n",
      "\n",
      "Epoch [81/100], Step [401/937], Loss: 0.1411 \n",
      "\n",
      "\n",
      "Epoch [81/100], Step [501/937], Loss: 0.0832 \n",
      "\n",
      "\n",
      "Epoch [81/100], Step [601/937], Loss: 0.1890 \n",
      "\n",
      "\n",
      "Epoch [81/100], Step [701/937], Loss: 0.0340 \n",
      "\n",
      "\n",
      "Epoch [81/100], Step [801/937], Loss: 0.1084 \n",
      "\n",
      "\n",
      "Epoch [81/100], Step [901/937], Loss: 0.0163 \n",
      "\n",
      "\n",
      "Epoch [82/100], Step [1/937], Loss: 0.1217 \n",
      "\n",
      "\n",
      "Epoch [82/100], Step [101/937], Loss: 0.1057 \n",
      "\n",
      "\n",
      "Epoch [82/100], Step [201/937], Loss: 0.0241 \n",
      "\n",
      "\n",
      "Epoch [82/100], Step [301/937], Loss: 0.0907 \n",
      "\n",
      "\n",
      "Epoch [82/100], Step [401/937], Loss: 0.1379 \n",
      "\n",
      "\n",
      "Epoch [82/100], Step [501/937], Loss: 0.0605 \n",
      "\n",
      "\n",
      "Epoch [82/100], Step [601/937], Loss: 0.0250 \n",
      "\n",
      "\n",
      "Epoch [82/100], Step [701/937], Loss: 0.0332 \n",
      "\n",
      "\n",
      "Epoch [82/100], Step [801/937], Loss: 0.0519 \n",
      "\n",
      "\n",
      "Epoch [82/100], Step [901/937], Loss: 0.1154 \n",
      "\n",
      "\n",
      "Epoch [83/100], Step [1/937], Loss: 0.0559 \n",
      "\n",
      "\n",
      "Epoch [83/100], Step [101/937], Loss: 0.0669 \n",
      "\n",
      "\n",
      "Epoch [83/100], Step [201/937], Loss: 0.1762 \n",
      "\n",
      "\n",
      "Epoch [83/100], Step [301/937], Loss: 0.0779 \n",
      "\n",
      "\n",
      "Epoch [83/100], Step [401/937], Loss: 0.0325 \n",
      "\n",
      "\n",
      "Epoch [83/100], Step [501/937], Loss: 0.0441 \n",
      "\n",
      "\n",
      "Epoch [83/100], Step [601/937], Loss: 0.1027 \n",
      "\n",
      "\n",
      "Epoch [83/100], Step [701/937], Loss: 0.0229 \n",
      "\n",
      "\n",
      "Epoch [83/100], Step [801/937], Loss: 0.2034 \n",
      "\n",
      "\n",
      "Epoch [83/100], Step [901/937], Loss: 0.0506 \n",
      "\n",
      "\n",
      "Epoch [84/100], Step [1/937], Loss: 0.0340 \n",
      "\n",
      "\n",
      "Epoch [84/100], Step [101/937], Loss: 0.0601 \n",
      "\n",
      "\n",
      "Epoch [84/100], Step [201/937], Loss: 0.0688 \n",
      "\n",
      "\n",
      "Epoch [84/100], Step [301/937], Loss: 0.2611 \n",
      "\n",
      "\n",
      "Epoch [84/100], Step [401/937], Loss: 0.0832 \n",
      "\n",
      "\n",
      "Epoch [84/100], Step [501/937], Loss: 0.0555 \n",
      "\n",
      "\n",
      "Epoch [84/100], Step [601/937], Loss: 0.0345 \n",
      "\n",
      "\n",
      "Epoch [84/100], Step [701/937], Loss: 0.0115 \n",
      "\n",
      "\n",
      "Epoch [84/100], Step [801/937], Loss: 0.0344 \n",
      "\n",
      "\n",
      "Epoch [84/100], Step [901/937], Loss: 0.1121 \n",
      "\n",
      "\n",
      "Epoch [85/100], Step [1/937], Loss: 0.0059 \n",
      "\n",
      "\n",
      "Epoch [85/100], Step [101/937], Loss: 0.0260 \n",
      "\n",
      "\n",
      "Epoch [85/100], Step [201/937], Loss: 0.1187 \n",
      "\n",
      "\n",
      "Epoch [85/100], Step [301/937], Loss: 0.0388 \n",
      "\n",
      "\n",
      "Epoch [85/100], Step [401/937], Loss: 0.0306 \n",
      "\n",
      "\n",
      "Epoch [85/100], Step [501/937], Loss: 0.0109 \n",
      "\n",
      "\n",
      "Epoch [85/100], Step [601/937], Loss: 0.0224 \n",
      "\n",
      "\n",
      "Epoch [85/100], Step [701/937], Loss: 0.0183 \n",
      "\n",
      "\n",
      "Epoch [85/100], Step [801/937], Loss: 0.0371 \n",
      "\n",
      "\n",
      "Epoch [85/100], Step [901/937], Loss: 0.0086 \n",
      "\n",
      "\n",
      "Epoch [86/100], Step [1/937], Loss: 0.0508 \n",
      "\n",
      "\n",
      "Epoch [86/100], Step [101/937], Loss: 0.0488 \n",
      "\n",
      "\n",
      "Epoch [86/100], Step [201/937], Loss: 0.0352 \n",
      "\n",
      "\n",
      "Epoch [86/100], Step [301/937], Loss: 0.1004 \n",
      "\n",
      "\n",
      "Epoch [86/100], Step [401/937], Loss: 0.1277 \n",
      "\n",
      "\n",
      "Epoch [86/100], Step [501/937], Loss: 0.0758 \n",
      "\n",
      "\n",
      "Epoch [86/100], Step [601/937], Loss: 0.0645 \n",
      "\n",
      "\n",
      "Epoch [86/100], Step [701/937], Loss: 0.0040 \n",
      "\n",
      "\n",
      "Epoch [86/100], Step [801/937], Loss: 0.1545 \n",
      "\n",
      "\n",
      "Epoch [86/100], Step [901/937], Loss: 0.0550 \n",
      "\n",
      "\n",
      "Epoch [87/100], Step [1/937], Loss: 0.0273 \n",
      "\n",
      "\n",
      "Epoch [87/100], Step [101/937], Loss: 0.0324 \n",
      "\n",
      "\n",
      "Epoch [87/100], Step [201/937], Loss: 0.1852 \n",
      "\n",
      "\n",
      "Epoch [87/100], Step [301/937], Loss: 0.0378 \n",
      "\n",
      "\n",
      "Epoch [87/100], Step [401/937], Loss: 0.0651 \n",
      "\n",
      "\n",
      "Epoch [87/100], Step [501/937], Loss: 0.0110 \n",
      "\n",
      "\n",
      "Epoch [87/100], Step [601/937], Loss: 0.1180 \n",
      "\n",
      "\n",
      "Epoch [87/100], Step [701/937], Loss: 0.0493 \n",
      "\n",
      "\n",
      "Epoch [87/100], Step [801/937], Loss: 0.0441 \n",
      "\n",
      "\n",
      "Epoch [87/100], Step [901/937], Loss: 0.1859 \n",
      "\n",
      "\n",
      "Epoch [88/100], Step [1/937], Loss: 0.0267 \n",
      "\n",
      "\n",
      "Epoch [88/100], Step [101/937], Loss: 0.0629 \n",
      "\n",
      "\n",
      "Epoch [88/100], Step [201/937], Loss: 0.0179 \n",
      "\n",
      "\n",
      "Epoch [88/100], Step [301/937], Loss: 0.1235 \n",
      "\n",
      "\n",
      "Epoch [88/100], Step [401/937], Loss: 0.0319 \n",
      "\n",
      "\n",
      "Epoch [88/100], Step [501/937], Loss: 0.0569 \n",
      "\n",
      "\n",
      "Epoch [88/100], Step [601/937], Loss: 0.0960 \n",
      "\n",
      "\n",
      "Epoch [88/100], Step [701/937], Loss: 0.0840 \n",
      "\n",
      "\n",
      "Epoch [88/100], Step [801/937], Loss: 0.1134 \n",
      "\n",
      "\n",
      "Epoch [88/100], Step [901/937], Loss: 0.0509 \n",
      "\n",
      "\n",
      "Epoch [89/100], Step [1/937], Loss: 0.2695 \n",
      "\n",
      "\n",
      "Epoch [89/100], Step [101/937], Loss: 0.0419 \n",
      "\n",
      "\n",
      "Epoch [89/100], Step [201/937], Loss: 0.1707 \n",
      "\n",
      "\n",
      "Epoch [89/100], Step [301/937], Loss: 0.0568 \n",
      "\n",
      "\n",
      "Epoch [89/100], Step [401/937], Loss: 0.0704 \n",
      "\n",
      "\n",
      "Epoch [89/100], Step [501/937], Loss: 0.1497 \n",
      "\n",
      "\n",
      "Epoch [89/100], Step [601/937], Loss: 0.0148 \n",
      "\n",
      "\n",
      "Epoch [89/100], Step [701/937], Loss: 0.0912 \n",
      "\n",
      "\n",
      "Epoch [89/100], Step [801/937], Loss: 0.0596 \n",
      "\n",
      "\n",
      "Epoch [89/100], Step [901/937], Loss: 0.0453 \n",
      "\n",
      "\n",
      "Epoch [90/100], Step [1/937], Loss: 0.0388 \n",
      "\n",
      "\n",
      "Epoch [90/100], Step [101/937], Loss: 0.0685 \n",
      "\n",
      "\n",
      "Epoch [90/100], Step [201/937], Loss: 0.0241 \n",
      "\n",
      "\n",
      "Epoch [90/100], Step [301/937], Loss: 0.0687 \n",
      "\n",
      "\n",
      "Epoch [90/100], Step [401/937], Loss: 0.0783 \n",
      "\n",
      "\n",
      "Epoch [90/100], Step [501/937], Loss: 0.0245 \n",
      "\n",
      "\n",
      "Epoch [90/100], Step [601/937], Loss: 0.0322 \n",
      "\n",
      "\n",
      "Epoch [90/100], Step [701/937], Loss: 0.0401 \n",
      "\n",
      "\n",
      "Epoch [90/100], Step [801/937], Loss: 0.0363 \n",
      "\n",
      "\n",
      "Epoch [90/100], Step [901/937], Loss: 0.0971 \n",
      "\n",
      "\n",
      "Epoch [91/100], Step [1/937], Loss: 0.0971 \n",
      "\n",
      "\n",
      "Epoch [91/100], Step [101/937], Loss: 0.0751 \n",
      "\n",
      "\n",
      "Epoch [91/100], Step [201/937], Loss: 0.0198 \n",
      "\n",
      "\n",
      "Epoch [91/100], Step [301/937], Loss: 0.0953 \n",
      "\n",
      "\n",
      "Epoch [91/100], Step [401/937], Loss: 0.1172 \n",
      "\n",
      "\n",
      "Epoch [91/100], Step [501/937], Loss: 0.0097 \n",
      "\n",
      "\n",
      "Epoch [91/100], Step [601/937], Loss: 0.0853 \n",
      "\n",
      "\n",
      "Epoch [91/100], Step [701/937], Loss: 0.0765 \n",
      "\n",
      "\n",
      "Epoch [91/100], Step [801/937], Loss: 0.0375 \n",
      "\n",
      "\n",
      "Epoch [91/100], Step [901/937], Loss: 0.0186 \n",
      "\n",
      "\n",
      "Epoch [92/100], Step [1/937], Loss: 0.1013 \n",
      "\n",
      "\n",
      "Epoch [92/100], Step [101/937], Loss: 0.1559 \n",
      "\n",
      "\n",
      "Epoch [92/100], Step [201/937], Loss: 0.1434 \n",
      "\n",
      "\n",
      "Epoch [92/100], Step [301/937], Loss: 0.0580 \n",
      "\n",
      "\n",
      "Epoch [92/100], Step [401/937], Loss: 0.0592 \n",
      "\n",
      "\n",
      "Epoch [92/100], Step [501/937], Loss: 0.0257 \n",
      "\n",
      "\n",
      "Epoch [92/100], Step [601/937], Loss: 0.0647 \n",
      "\n",
      "\n",
      "Epoch [92/100], Step [701/937], Loss: 0.0203 \n",
      "\n",
      "\n",
      "Epoch [92/100], Step [801/937], Loss: 0.1167 \n",
      "\n",
      "\n",
      "Epoch [92/100], Step [901/937], Loss: 0.0099 \n",
      "\n",
      "\n",
      "Epoch [93/100], Step [1/937], Loss: 0.1070 \n",
      "\n",
      "\n",
      "Epoch [93/100], Step [101/937], Loss: 0.1028 \n",
      "\n",
      "\n",
      "Epoch [93/100], Step [201/937], Loss: 0.1751 \n",
      "\n",
      "\n",
      "Epoch [93/100], Step [301/937], Loss: 0.0322 \n",
      "\n",
      "\n",
      "Epoch [93/100], Step [401/937], Loss: 0.0460 \n",
      "\n",
      "\n",
      "Epoch [93/100], Step [501/937], Loss: 0.0078 \n",
      "\n",
      "\n",
      "Epoch [93/100], Step [601/937], Loss: 0.0568 \n",
      "\n",
      "\n",
      "Epoch [93/100], Step [701/937], Loss: 0.0184 \n",
      "\n",
      "\n",
      "Epoch [93/100], Step [801/937], Loss: 0.0616 \n",
      "\n",
      "\n",
      "Epoch [93/100], Step [901/937], Loss: 0.1445 \n",
      "\n",
      "\n",
      "Epoch [94/100], Step [1/937], Loss: 0.2115 \n",
      "\n",
      "\n",
      "Epoch [94/100], Step [101/937], Loss: 0.1230 \n",
      "\n",
      "\n",
      "Epoch [94/100], Step [201/937], Loss: 0.0188 \n",
      "\n",
      "\n",
      "Epoch [94/100], Step [301/937], Loss: 0.0370 \n",
      "\n",
      "\n",
      "Epoch [94/100], Step [401/937], Loss: 0.0268 \n",
      "\n",
      "\n",
      "Epoch [94/100], Step [501/937], Loss: 0.0045 \n",
      "\n",
      "\n",
      "Epoch [94/100], Step [601/937], Loss: 0.1059 \n",
      "\n",
      "\n",
      "Epoch [94/100], Step [701/937], Loss: 0.0650 \n",
      "\n",
      "\n",
      "Epoch [94/100], Step [801/937], Loss: 0.1793 \n",
      "\n",
      "\n",
      "Epoch [94/100], Step [901/937], Loss: 0.0869 \n",
      "\n",
      "\n",
      "Epoch [95/100], Step [1/937], Loss: 0.0645 \n",
      "\n",
      "\n",
      "Epoch [95/100], Step [101/937], Loss: 0.2375 \n",
      "\n",
      "\n",
      "Epoch [95/100], Step [201/937], Loss: 0.0888 \n",
      "\n",
      "\n",
      "Epoch [95/100], Step [301/937], Loss: 0.1028 \n",
      "\n",
      "\n",
      "Epoch [95/100], Step [401/937], Loss: 0.0384 \n",
      "\n",
      "\n",
      "Epoch [95/100], Step [501/937], Loss: 0.0191 \n",
      "\n",
      "\n",
      "Epoch [95/100], Step [601/937], Loss: 0.1092 \n",
      "\n",
      "\n",
      "Epoch [95/100], Step [701/937], Loss: 0.0403 \n",
      "\n",
      "\n",
      "Epoch [95/100], Step [801/937], Loss: 0.1774 \n",
      "\n",
      "\n",
      "Epoch [95/100], Step [901/937], Loss: 0.0421 \n",
      "\n",
      "\n",
      "Epoch [96/100], Step [1/937], Loss: 0.0242 \n",
      "\n",
      "\n",
      "Epoch [96/100], Step [101/937], Loss: 0.1297 \n",
      "\n",
      "\n",
      "Epoch [96/100], Step [201/937], Loss: 0.0654 \n",
      "\n",
      "\n",
      "Epoch [96/100], Step [301/937], Loss: 0.0081 \n",
      "\n",
      "\n",
      "Epoch [96/100], Step [401/937], Loss: 0.1178 \n",
      "\n",
      "\n",
      "Epoch [96/100], Step [501/937], Loss: 0.1059 \n",
      "\n",
      "\n",
      "Epoch [96/100], Step [601/937], Loss: 0.1224 \n",
      "\n",
      "\n",
      "Epoch [96/100], Step [701/937], Loss: 0.0518 \n",
      "\n",
      "\n",
      "Epoch [96/100], Step [801/937], Loss: 0.0059 \n",
      "\n",
      "\n",
      "Epoch [96/100], Step [901/937], Loss: 0.0946 \n",
      "\n",
      "\n",
      "Epoch [97/100], Step [1/937], Loss: 0.0469 \n",
      "\n",
      "\n",
      "Epoch [97/100], Step [101/937], Loss: 0.0900 \n",
      "\n",
      "\n",
      "Epoch [97/100], Step [201/937], Loss: 0.2005 \n",
      "\n",
      "\n",
      "Epoch [97/100], Step [301/937], Loss: 0.0292 \n",
      "\n",
      "\n",
      "Epoch [97/100], Step [401/937], Loss: 0.0136 \n",
      "\n",
      "\n",
      "Epoch [97/100], Step [501/937], Loss: 0.0542 \n",
      "\n",
      "\n",
      "Epoch [97/100], Step [601/937], Loss: 0.0469 \n",
      "\n",
      "\n",
      "Epoch [97/100], Step [701/937], Loss: 0.0467 \n",
      "\n",
      "\n",
      "Epoch [97/100], Step [801/937], Loss: 0.0244 \n",
      "\n",
      "\n",
      "Epoch [97/100], Step [901/937], Loss: 0.0751 \n",
      "\n",
      "\n",
      "Epoch [98/100], Step [1/937], Loss: 0.0294 \n",
      "\n",
      "\n",
      "Epoch [98/100], Step [101/937], Loss: 0.1831 \n",
      "\n",
      "\n",
      "Epoch [98/100], Step [201/937], Loss: 0.0202 \n",
      "\n",
      "\n",
      "Epoch [98/100], Step [301/937], Loss: 0.0907 \n",
      "\n",
      "\n",
      "Epoch [98/100], Step [401/937], Loss: 0.0473 \n",
      "\n",
      "\n",
      "Epoch [98/100], Step [501/937], Loss: 0.0524 \n",
      "\n",
      "\n",
      "Epoch [98/100], Step [601/937], Loss: 0.1495 \n",
      "\n",
      "\n",
      "Epoch [98/100], Step [701/937], Loss: 0.0480 \n",
      "\n",
      "\n",
      "Epoch [98/100], Step [801/937], Loss: 0.0669 \n",
      "\n",
      "\n",
      "Epoch [98/100], Step [901/937], Loss: 0.0377 \n",
      "\n",
      "\n",
      "Epoch [99/100], Step [1/937], Loss: 0.0158 \n",
      "\n",
      "\n",
      "Epoch [99/100], Step [101/937], Loss: 0.1142 \n",
      "\n",
      "\n",
      "Epoch [99/100], Step [201/937], Loss: 0.1628 \n",
      "\n",
      "\n",
      "Epoch [99/100], Step [301/937], Loss: 0.1189 \n",
      "\n",
      "\n",
      "Epoch [99/100], Step [401/937], Loss: 0.0680 \n",
      "\n",
      "\n",
      "Epoch [99/100], Step [501/937], Loss: 0.1044 \n",
      "\n",
      "\n",
      "Epoch [99/100], Step [601/937], Loss: 0.0207 \n",
      "\n",
      "\n",
      "Epoch [99/100], Step [701/937], Loss: 0.0269 \n",
      "\n",
      "\n",
      "Epoch [99/100], Step [801/937], Loss: 0.0568 \n",
      "\n",
      "\n",
      "Epoch [99/100], Step [901/937], Loss: 0.0399 \n",
      "\n",
      "\n",
      "Epoch [100/100], Step [1/937], Loss: 0.0522 \n",
      "\n",
      "\n",
      "Epoch [100/100], Step [101/937], Loss: 0.0038 \n",
      "\n",
      "\n",
      "Epoch [100/100], Step [201/937], Loss: 0.0458 \n",
      "\n",
      "\n",
      "Epoch [100/100], Step [301/937], Loss: 0.0874 \n",
      "\n",
      "\n",
      "Epoch [100/100], Step [401/937], Loss: 0.0519 \n",
      "\n",
      "\n",
      "Epoch [100/100], Step [501/937], Loss: 0.0703 \n",
      "\n",
      "\n",
      "Epoch [100/100], Step [601/937], Loss: 0.1778 \n",
      "\n",
      "\n",
      "Epoch [100/100], Step [701/937], Loss: 0.0372 \n",
      "\n",
      "\n",
      "Epoch [100/100], Step [801/937], Loss: 0.1941 \n",
      "\n",
      "\n",
      "Epoch [100/100], Step [901/937], Loss: 0.0535 \n",
      "\n",
      "\n",
      "Total training time: 504.62978529930115 seconds.\n"
     ]
    }
   ],
   "source": [
    "# CNN Training\n",
    "\n",
    "num_of_iterations = X_train.shape[0] // batch_size\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        images = images.float()\n",
    "        images = images.unsqueeze(1)\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = cnn_model.forward(images)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = error(outputs, labels)\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        count += 1\n",
    "        loss_list.append(loss.data)\n",
    "        iteration_list.append(count)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{total_epoch}], Step [{i + 1}/{num_of_iterations}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "training_end_time = time.time()\n",
    "\n",
    "print(f\"\\nTotal training time: {(training_end_time - training_start_time)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be86a902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss Visualization\n",
    "\n",
    "plt.plot(iteration_list, loss_list)\n",
    "\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.title(\"CNN: Training Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f453402",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88b8d8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic: 2051, Number of Images: 10000, Rows: 28, Columns: 28\n",
      "Magic: 2049, Number of Labels: 10000\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_mnist_images_manual(\"dataset/t10k-images.idx3-ubyte\", \"dataset/t10k-labels.idx1-ubyte\")\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d5c897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test Shape: torch.Size([10000, 28, 28])\n",
      "y_test Shape: tensor([7, 2, 1,  ..., 4, 5, 6], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_test Shape: {X_test.shape}\")\n",
    "print(f\"y_test Shape: {y_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce1466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(X_test, y_test)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4685755f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct predictions: 9753 \n",
      " Total Samples: 10000\n",
      "Accuracy: 97.530\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(test_dataloader):\n",
    "        images = images.float()\n",
    "        images = images.unsqueeze(1)\n",
    "        \n",
    "        outputs = cnn_model.forward(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        total_correct += (predictions == labels).sum().item()\n",
    "        total_samples += labels.shape[0]\n",
    "\n",
    "print(f\"Total correct predictions: {total_correct} \\n Total Samples: {total_samples}\")\n",
    "print(f\"Accuracy: {total_correct / total_samples * 100:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
