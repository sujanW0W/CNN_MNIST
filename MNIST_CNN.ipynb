{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ff3d15",
   "metadata": {},
   "source": [
    "# MNIST Classification using Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab5117",
   "metadata": {},
   "source": [
    "This is an introductory CNN project that classifies MNIST data.\n",
    "\n",
    "The dataset consists of images of size 28 * 28. So, the input to CNN is 28 * 28 image. We will build 2 convolutional layers, each followed by a pooling layer. So, the architecture I will be using is as follows:\n",
    "\n",
    "Input Layer -> ConvLayer_1 -> MaxPooling_1 -> ConvLayer_2 -> MaxPooling_2 -> Flattened Feature Map -> Fully Connected Layer -> Output (Class scores)\n",
    "\n",
    "I will use 3 * 3 Filter size for first ConvLayer and 3 * 3 * 3 (depth of 3 to match 3 channels) with a stride of 1 and no padding (might change based on output) and 2 * 2 Pooling size with stride of 2. Also, each ConvLayer will have 3 filters which generate 3 channels of feature map. Finally, the feature map from last pooling layer is flattened and fed to the fully connected layer. The fully connected layer will perform same operation as in feed-forward neural networks. I will use RELU activation for hidden layer and Softmax for output layer.\n",
    "\n",
    "The size of feature map can be calculated as: \n",
    "((Input Size - Receptive Field Size) + 2 * Zero Padding) / (Stride + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8904c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2404cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ffd729",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f21aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_images_manual(image_path, label_path):\n",
    "    with open (image_path, 'rb') as img_file:\n",
    "        # Read the header\n",
    "        magic, num_images, rows, cols = struct.unpack('>IIII', img_file.read(16))\n",
    "\n",
    "        print(f\"Magic: {magic}, Number of Images: {num_images}, Rows: {rows}, Columns: {cols}\")\n",
    "\n",
    "        # Read the image data\n",
    "        image_data = img_file.read(rows * cols * num_images)\n",
    "        images = torch.frombuffer(image_data, dtype=torch.uint8)\n",
    "\n",
    "        images = images.reshape((num_images, rows, cols))\n",
    "\n",
    "    with open (label_path, 'rb') as lbl_file:\n",
    "        # Read the header\n",
    "        magic, num_labels = struct.unpack('>II', lbl_file.read(8))\n",
    "\n",
    "        print(f\"Magic: {magic}, Number of Labels: {num_labels}\")\n",
    "\n",
    "        # Read the label data\n",
    "        label_data = lbl_file.read(num_labels)\n",
    "        labels = torch.frombuffer(label_data, dtype=torch.uint8)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc57c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some random images\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def display_images(images, labels):\n",
    "    cols = 4\n",
    "    rows = math.ceil(len(images) / cols)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    index = 0\n",
    "\n",
    "    for x in zip(images, labels):\n",
    "        image = x[0]\n",
    "        label = x[1]\n",
    "        plt.subplot(rows, cols, index + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5caef15",
   "metadata": {},
   "source": [
    "## Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30bc8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4434f",
   "metadata": {},
   "source": [
    "## Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d426410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Convolution Layer 1\n",
    "        self.conv_layer_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu_1 = nn.ReLU()\n",
    " \n",
    "        # Max Pooling 1\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Convolution Layer 2\n",
    "        self.conv_layer_2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "\n",
    "        # Max Pooling 2\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Flattening layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        # Fully Connected 1\n",
    "        self.fc_1 = nn.Linear(in_features=3 * 4 * 4, out_features=24)\n",
    "        self.fc_1_relu = nn.ReLU()\n",
    "\n",
    "        # Fully Connected 2\n",
    "        self.fc_2 = nn.Linear(in_features=24, out_features=10)\n",
    "        self.fc_2_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Convolution 1\n",
    "        out = self.conv_layer_1(X)\n",
    "        out = self.relu_1(out)\n",
    "\n",
    "        # Max Pooling 1\n",
    "        out = self.max_pool_1(out)\n",
    "\n",
    "        # Convolution 2\n",
    "        out = self.conv_layer_2(out)\n",
    "        out = self.relu_2(out)\n",
    "\n",
    "        # Max Pooling 2\n",
    "        out = self.max_pool_2(out)\n",
    "\n",
    "        # Flatten\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Fully Connected 1\n",
    "        out = self.fc_1(out)\n",
    "        out = self.fc_1_relu(out)\n",
    "\n",
    "        # Fully Connected 2\n",
    "        out = self.fc_2(out)\n",
    "        out = self.fc_2_softmax(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f8fcc",
   "metadata": {},
   "source": [
    "## Initialize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8fe0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "total_epoch = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07831cf3",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f9f3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic: 2051, Number of Images: 60000, Rows: 28, Columns: 28\n",
      "Magic: 2049, Number of Labels: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sujan\\AppData\\Local\\Temp\\ipykernel_23212\\2397727395.py:10: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:1567.)\n",
      "  images = torch.frombuffer(image_data, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist_images_manual(\"dataset/train-images.idx3-ubyte\", \"dataset/train-labels.idx1-ubyte\")\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01244d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([60000, 28, 28])\n",
      "y_train shape: torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c735f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(X_train, y_train)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c42bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e5dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab2bc7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [1/937], Loss: 2.3204\n",
      "Epoch [1/100], Step [101/937], Loss: 2.2687\n",
      "Epoch [1/100], Step [201/937], Loss: 2.2226\n",
      "Epoch [1/100], Step [301/937], Loss: 2.1578\n",
      "Epoch [1/100], Step [401/937], Loss: 2.2235\n",
      "Epoch [1/100], Step [501/937], Loss: 2.2117\n",
      "Epoch [1/100], Step [601/937], Loss: 2.1311\n",
      "Epoch [1/100], Step [701/937], Loss: 2.0847\n",
      "Epoch [1/100], Step [801/937], Loss: 2.1447\n",
      "Epoch [1/100], Step [901/937], Loss: 2.0816\n",
      "Epoch [2/100], Step [1/937], Loss: 2.0511\n",
      "Epoch [2/100], Step [101/937], Loss: 2.0302\n",
      "Epoch [2/100], Step [201/937], Loss: 2.0217\n",
      "Epoch [2/100], Step [301/937], Loss: 2.1595\n",
      "Epoch [2/100], Step [401/937], Loss: 2.0596\n",
      "Epoch [2/100], Step [501/937], Loss: 1.9959\n",
      "Epoch [2/100], Step [601/937], Loss: 1.9994\n",
      "Epoch [2/100], Step [701/937], Loss: 2.0761\n",
      "Epoch [2/100], Step [801/937], Loss: 2.0341\n",
      "Epoch [2/100], Step [901/937], Loss: 2.0511\n",
      "Epoch [3/100], Step [1/937], Loss: 1.9456\n",
      "Epoch [3/100], Step [101/937], Loss: 1.9844\n",
      "Epoch [3/100], Step [201/937], Loss: 1.8690\n",
      "Epoch [3/100], Step [301/937], Loss: 1.9931\n",
      "Epoch [3/100], Step [401/937], Loss: 1.9059\n",
      "Epoch [3/100], Step [501/937], Loss: 1.9615\n",
      "Epoch [3/100], Step [601/937], Loss: 2.0592\n",
      "Epoch [3/100], Step [701/937], Loss: 1.9040\n",
      "Epoch [3/100], Step [801/937], Loss: 2.0557\n",
      "Epoch [3/100], Step [901/937], Loss: 2.0091\n",
      "Epoch [4/100], Step [1/937], Loss: 1.9522\n",
      "Epoch [4/100], Step [101/937], Loss: 2.0126\n",
      "Epoch [4/100], Step [201/937], Loss: 1.9686\n",
      "Epoch [4/100], Step [301/937], Loss: 2.1075\n",
      "Epoch [4/100], Step [401/937], Loss: 1.9670\n",
      "Epoch [4/100], Step [501/937], Loss: 1.9471\n",
      "Epoch [4/100], Step [601/937], Loss: 2.0743\n",
      "Epoch [4/100], Step [701/937], Loss: 2.0259\n",
      "Epoch [4/100], Step [801/937], Loss: 2.0034\n",
      "Epoch [4/100], Step [901/937], Loss: 1.9682\n",
      "Epoch [5/100], Step [1/937], Loss: 1.9879\n",
      "Epoch [5/100], Step [101/937], Loss: 1.9008\n",
      "Epoch [5/100], Step [201/937], Loss: 1.8871\n",
      "Epoch [5/100], Step [301/937], Loss: 2.0242\n",
      "Epoch [5/100], Step [401/937], Loss: 1.9605\n",
      "Epoch [5/100], Step [501/937], Loss: 2.0296\n",
      "Epoch [5/100], Step [601/937], Loss: 2.0236\n",
      "Epoch [5/100], Step [701/937], Loss: 1.9413\n",
      "Epoch [5/100], Step [801/937], Loss: 2.0050\n",
      "Epoch [5/100], Step [901/937], Loss: 1.9308\n",
      "Epoch [6/100], Step [1/937], Loss: 1.9992\n",
      "Epoch [6/100], Step [101/937], Loss: 1.9400\n",
      "Epoch [6/100], Step [201/937], Loss: 2.0184\n",
      "Epoch [6/100], Step [301/937], Loss: 1.9502\n",
      "Epoch [6/100], Step [401/937], Loss: 1.9743\n",
      "Epoch [6/100], Step [501/937], Loss: 1.9000\n",
      "Epoch [6/100], Step [601/937], Loss: 1.9779\n",
      "Epoch [6/100], Step [701/937], Loss: 1.9244\n",
      "Epoch [6/100], Step [801/937], Loss: 2.0282\n",
      "Epoch [6/100], Step [901/937], Loss: 1.9154\n",
      "Epoch [7/100], Step [1/937], Loss: 1.9187\n",
      "Epoch [7/100], Step [101/937], Loss: 2.0276\n",
      "Epoch [7/100], Step [201/937], Loss: 2.0258\n",
      "Epoch [7/100], Step [301/937], Loss: 1.8222\n",
      "Epoch [7/100], Step [401/937], Loss: 1.9440\n",
      "Epoch [7/100], Step [501/937], Loss: 2.0239\n",
      "Epoch [7/100], Step [601/937], Loss: 2.0772\n",
      "Epoch [7/100], Step [701/937], Loss: 1.8847\n",
      "Epoch [7/100], Step [801/937], Loss: 1.9740\n",
      "Epoch [7/100], Step [901/937], Loss: 1.9572\n",
      "Epoch [8/100], Step [1/937], Loss: 1.8789\n",
      "Epoch [8/100], Step [101/937], Loss: 1.9640\n",
      "Epoch [8/100], Step [201/937], Loss: 1.9228\n",
      "Epoch [8/100], Step [301/937], Loss: 2.0025\n",
      "Epoch [8/100], Step [401/937], Loss: 1.8767\n",
      "Epoch [8/100], Step [501/937], Loss: 2.0191\n",
      "Epoch [8/100], Step [601/937], Loss: 2.0529\n",
      "Epoch [8/100], Step [701/937], Loss: 2.0479\n",
      "Epoch [8/100], Step [801/937], Loss: 1.9550\n",
      "Epoch [8/100], Step [901/937], Loss: 1.8662\n",
      "Epoch [9/100], Step [1/937], Loss: 2.0685\n",
      "Epoch [9/100], Step [101/937], Loss: 1.8986\n",
      "Epoch [9/100], Step [201/937], Loss: 2.0172\n",
      "Epoch [9/100], Step [301/937], Loss: 1.9405\n",
      "Epoch [9/100], Step [401/937], Loss: 2.0052\n",
      "Epoch [9/100], Step [501/937], Loss: 1.9341\n",
      "Epoch [9/100], Step [601/937], Loss: 1.9396\n",
      "Epoch [9/100], Step [701/937], Loss: 1.9081\n",
      "Epoch [9/100], Step [801/937], Loss: 1.9698\n",
      "Epoch [9/100], Step [901/937], Loss: 2.0614\n",
      "Epoch [10/100], Step [1/937], Loss: 1.9846\n",
      "Epoch [10/100], Step [101/937], Loss: 1.9313\n",
      "Epoch [10/100], Step [201/937], Loss: 2.0117\n",
      "Epoch [10/100], Step [301/937], Loss: 1.9727\n",
      "Epoch [10/100], Step [401/937], Loss: 1.9878\n",
      "Epoch [10/100], Step [501/937], Loss: 1.8944\n",
      "Epoch [10/100], Step [601/937], Loss: 1.8895\n",
      "Epoch [10/100], Step [701/937], Loss: 1.9084\n",
      "Epoch [10/100], Step [801/937], Loss: 1.8667\n",
      "Epoch [10/100], Step [901/937], Loss: 1.9742\n",
      "Epoch [11/100], Step [1/937], Loss: 1.9274\n",
      "Epoch [11/100], Step [101/937], Loss: 2.0343\n",
      "Epoch [11/100], Step [201/937], Loss: 1.9240\n",
      "Epoch [11/100], Step [301/937], Loss: 1.8926\n",
      "Epoch [11/100], Step [401/937], Loss: 1.9542\n",
      "Epoch [11/100], Step [501/937], Loss: 1.8946\n",
      "Epoch [11/100], Step [601/937], Loss: 1.9887\n",
      "Epoch [11/100], Step [701/937], Loss: 1.9565\n",
      "Epoch [11/100], Step [801/937], Loss: 1.9313\n",
      "Epoch [11/100], Step [901/937], Loss: 2.0010\n",
      "Epoch [12/100], Step [1/937], Loss: 1.9739\n",
      "Epoch [12/100], Step [101/937], Loss: 2.0143\n",
      "Epoch [12/100], Step [201/937], Loss: 2.0721\n",
      "Epoch [12/100], Step [301/937], Loss: 1.8943\n",
      "Epoch [12/100], Step [401/937], Loss: 1.9247\n",
      "Epoch [12/100], Step [501/937], Loss: 1.9855\n",
      "Epoch [12/100], Step [601/937], Loss: 2.0171\n",
      "Epoch [12/100], Step [701/937], Loss: 2.0048\n",
      "Epoch [12/100], Step [801/937], Loss: 1.9841\n",
      "Epoch [12/100], Step [901/937], Loss: 1.9573\n",
      "Epoch [13/100], Step [1/937], Loss: 1.9000\n",
      "Epoch [13/100], Step [101/937], Loss: 1.9565\n",
      "Epoch [13/100], Step [201/937], Loss: 1.9412\n",
      "Epoch [13/100], Step [301/937], Loss: 1.9011\n",
      "Epoch [13/100], Step [401/937], Loss: 1.9067\n",
      "Epoch [13/100], Step [501/937], Loss: 1.9521\n",
      "Epoch [13/100], Step [601/937], Loss: 1.8633\n",
      "Epoch [13/100], Step [701/937], Loss: 2.0216\n",
      "Epoch [13/100], Step [801/937], Loss: 1.9522\n",
      "Epoch [13/100], Step [901/937], Loss: 1.9866\n",
      "Epoch [14/100], Step [1/937], Loss: 1.9427\n",
      "Epoch [14/100], Step [101/937], Loss: 1.9488\n",
      "Epoch [14/100], Step [201/937], Loss: 1.9639\n",
      "Epoch [14/100], Step [301/937], Loss: 1.8808\n",
      "Epoch [14/100], Step [401/937], Loss: 1.9881\n",
      "Epoch [14/100], Step [501/937], Loss: 2.0184\n",
      "Epoch [14/100], Step [601/937], Loss: 1.9688\n",
      "Epoch [14/100], Step [701/937], Loss: 1.9125\n",
      "Epoch [14/100], Step [801/937], Loss: 2.0169\n",
      "Epoch [14/100], Step [901/937], Loss: 1.9926\n",
      "Epoch [15/100], Step [1/937], Loss: 2.0038\n",
      "Epoch [15/100], Step [101/937], Loss: 1.9541\n",
      "Epoch [15/100], Step [201/937], Loss: 1.9393\n",
      "Epoch [15/100], Step [301/937], Loss: 1.9449\n",
      "Epoch [15/100], Step [401/937], Loss: 1.9285\n",
      "Epoch [15/100], Step [501/937], Loss: 1.9930\n",
      "Epoch [15/100], Step [601/937], Loss: 1.9702\n",
      "Epoch [15/100], Step [701/937], Loss: 1.9882\n",
      "Epoch [15/100], Step [801/937], Loss: 1.9437\n",
      "Epoch [15/100], Step [901/937], Loss: 1.9774\n",
      "Epoch [16/100], Step [1/937], Loss: 1.9527\n",
      "Epoch [16/100], Step [101/937], Loss: 2.0050\n",
      "Epoch [16/100], Step [201/937], Loss: 2.0280\n",
      "Epoch [16/100], Step [301/937], Loss: 2.0604\n",
      "Epoch [16/100], Step [401/937], Loss: 1.9912\n",
      "Epoch [16/100], Step [501/937], Loss: 2.0851\n",
      "Epoch [16/100], Step [601/937], Loss: 1.9530\n",
      "Epoch [16/100], Step [701/937], Loss: 2.0036\n",
      "Epoch [16/100], Step [801/937], Loss: 2.0337\n",
      "Epoch [16/100], Step [901/937], Loss: 1.9468\n",
      "Epoch [17/100], Step [1/937], Loss: 1.9551\n",
      "Epoch [17/100], Step [101/937], Loss: 1.8852\n",
      "Epoch [17/100], Step [201/937], Loss: 1.9480\n",
      "Epoch [17/100], Step [301/937], Loss: 1.9187\n",
      "Epoch [17/100], Step [401/937], Loss: 1.9103\n",
      "Epoch [17/100], Step [501/937], Loss: 1.9682\n",
      "Epoch [17/100], Step [601/937], Loss: 2.0487\n",
      "Epoch [17/100], Step [701/937], Loss: 1.9211\n",
      "Epoch [17/100], Step [801/937], Loss: 1.9748\n",
      "Epoch [17/100], Step [901/937], Loss: 2.0402\n",
      "Epoch [18/100], Step [1/937], Loss: 1.8963\n",
      "Epoch [18/100], Step [101/937], Loss: 1.8983\n",
      "Epoch [18/100], Step [201/937], Loss: 1.9643\n",
      "Epoch [18/100], Step [301/937], Loss: 1.9444\n",
      "Epoch [18/100], Step [401/937], Loss: 1.8885\n",
      "Epoch [18/100], Step [501/937], Loss: 1.9578\n",
      "Epoch [18/100], Step [601/937], Loss: 1.8931\n",
      "Epoch [18/100], Step [701/937], Loss: 1.9238\n",
      "Epoch [18/100], Step [801/937], Loss: 2.0766\n",
      "Epoch [18/100], Step [901/937], Loss: 1.9887\n",
      "Epoch [19/100], Step [1/937], Loss: 1.9674\n",
      "Epoch [19/100], Step [101/937], Loss: 1.9531\n",
      "Epoch [19/100], Step [201/937], Loss: 1.9814\n",
      "Epoch [19/100], Step [301/937], Loss: 2.0003\n",
      "Epoch [19/100], Step [401/937], Loss: 1.9460\n",
      "Epoch [19/100], Step [501/937], Loss: 1.8750\n",
      "Epoch [19/100], Step [601/937], Loss: 2.0092\n",
      "Epoch [19/100], Step [701/937], Loss: 1.9834\n",
      "Epoch [19/100], Step [801/937], Loss: 2.0178\n",
      "Epoch [19/100], Step [901/937], Loss: 1.9248\n",
      "Epoch [20/100], Step [1/937], Loss: 1.9514\n",
      "Epoch [20/100], Step [101/937], Loss: 1.9070\n",
      "Epoch [20/100], Step [201/937], Loss: 1.8148\n",
      "Epoch [20/100], Step [301/937], Loss: 1.9615\n",
      "Epoch [20/100], Step [401/937], Loss: 1.8767\n",
      "Epoch [20/100], Step [501/937], Loss: 1.9765\n",
      "Epoch [20/100], Step [601/937], Loss: 2.0154\n",
      "Epoch [20/100], Step [701/937], Loss: 2.0150\n",
      "Epoch [20/100], Step [801/937], Loss: 1.8950\n",
      "Epoch [20/100], Step [901/937], Loss: 1.9286\n",
      "Epoch [21/100], Step [1/937], Loss: 1.8672\n",
      "Epoch [21/100], Step [101/937], Loss: 1.9681\n",
      "Epoch [21/100], Step [201/937], Loss: 2.0131\n",
      "Epoch [21/100], Step [301/937], Loss: 1.9239\n",
      "Epoch [21/100], Step [401/937], Loss: 1.9678\n",
      "Epoch [21/100], Step [501/937], Loss: 1.9700\n",
      "Epoch [21/100], Step [601/937], Loss: 1.9564\n",
      "Epoch [21/100], Step [701/937], Loss: 1.9709\n",
      "Epoch [21/100], Step [801/937], Loss: 1.9547\n",
      "Epoch [21/100], Step [901/937], Loss: 1.9108\n",
      "Epoch [22/100], Step [1/937], Loss: 1.9803\n",
      "Epoch [22/100], Step [101/937], Loss: 2.0000\n",
      "Epoch [22/100], Step [201/937], Loss: 1.8649\n",
      "Epoch [22/100], Step [301/937], Loss: 1.9714\n",
      "Epoch [22/100], Step [401/937], Loss: 2.0621\n",
      "Epoch [22/100], Step [501/937], Loss: 1.9980\n",
      "Epoch [22/100], Step [601/937], Loss: 1.8600\n",
      "Epoch [22/100], Step [701/937], Loss: 1.9351\n",
      "Epoch [22/100], Step [801/937], Loss: 1.9091\n",
      "Epoch [22/100], Step [901/937], Loss: 2.0338\n",
      "Epoch [23/100], Step [1/937], Loss: 1.9560\n",
      "Epoch [23/100], Step [101/937], Loss: 1.9003\n",
      "Epoch [23/100], Step [201/937], Loss: 1.9561\n",
      "Epoch [23/100], Step [301/937], Loss: 1.8853\n",
      "Epoch [23/100], Step [401/937], Loss: 1.9612\n",
      "Epoch [23/100], Step [501/937], Loss: 1.8995\n",
      "Epoch [23/100], Step [601/937], Loss: 2.0336\n",
      "Epoch [23/100], Step [701/937], Loss: 2.0100\n",
      "Epoch [23/100], Step [801/937], Loss: 1.9562\n",
      "Epoch [23/100], Step [901/937], Loss: 1.9736\n",
      "Epoch [24/100], Step [1/937], Loss: 1.9990\n",
      "Epoch [24/100], Step [101/937], Loss: 2.0175\n",
      "Epoch [24/100], Step [201/937], Loss: 1.9316\n",
      "Epoch [24/100], Step [301/937], Loss: 1.8640\n",
      "Epoch [24/100], Step [401/937], Loss: 1.9418\n",
      "Epoch [24/100], Step [501/937], Loss: 1.9298\n",
      "Epoch [24/100], Step [601/937], Loss: 1.8923\n",
      "Epoch [24/100], Step [701/937], Loss: 1.8641\n",
      "Epoch [24/100], Step [801/937], Loss: 2.0544\n",
      "Epoch [24/100], Step [901/937], Loss: 1.8482\n",
      "Epoch [25/100], Step [1/937], Loss: 1.9401\n",
      "Epoch [25/100], Step [101/937], Loss: 1.8916\n",
      "Epoch [25/100], Step [201/937], Loss: 1.9155\n",
      "Epoch [25/100], Step [301/937], Loss: 1.8889\n",
      "Epoch [25/100], Step [401/937], Loss: 1.9823\n",
      "Epoch [25/100], Step [501/937], Loss: 1.9005\n",
      "Epoch [25/100], Step [601/937], Loss: 1.9485\n",
      "Epoch [25/100], Step [701/937], Loss: 1.9677\n",
      "Epoch [25/100], Step [801/937], Loss: 1.8947\n",
      "Epoch [25/100], Step [901/937], Loss: 1.9938\n",
      "Epoch [26/100], Step [1/937], Loss: 2.0285\n",
      "Epoch [26/100], Step [101/937], Loss: 1.8789\n",
      "Epoch [26/100], Step [201/937], Loss: 1.8683\n",
      "Epoch [26/100], Step [301/937], Loss: 2.0432\n",
      "Epoch [26/100], Step [401/937], Loss: 1.9216\n",
      "Epoch [26/100], Step [501/937], Loss: 1.8688\n",
      "Epoch [26/100], Step [601/937], Loss: 1.8825\n",
      "Epoch [26/100], Step [701/937], Loss: 1.8971\n",
      "Epoch [26/100], Step [801/937], Loss: 2.0279\n",
      "Epoch [26/100], Step [901/937], Loss: 1.9392\n",
      "Epoch [27/100], Step [1/937], Loss: 2.0006\n",
      "Epoch [27/100], Step [101/937], Loss: 1.9129\n",
      "Epoch [27/100], Step [201/937], Loss: 1.8629\n",
      "Epoch [27/100], Step [301/937], Loss: 1.9639\n",
      "Epoch [27/100], Step [401/937], Loss: 1.9220\n",
      "Epoch [27/100], Step [501/937], Loss: 1.9522\n",
      "Epoch [27/100], Step [601/937], Loss: 1.8877\n",
      "Epoch [27/100], Step [701/937], Loss: 1.8494\n",
      "Epoch [27/100], Step [801/937], Loss: 1.8741\n",
      "Epoch [27/100], Step [901/937], Loss: 2.0214\n",
      "Epoch [28/100], Step [1/937], Loss: 1.8149\n",
      "Epoch [28/100], Step [101/937], Loss: 1.9123\n",
      "Epoch [28/100], Step [201/937], Loss: 1.9412\n",
      "Epoch [28/100], Step [301/937], Loss: 2.0312\n",
      "Epoch [28/100], Step [401/937], Loss: 1.9074\n",
      "Epoch [28/100], Step [501/937], Loss: 1.9885\n",
      "Epoch [28/100], Step [601/937], Loss: 2.0214\n",
      "Epoch [28/100], Step [701/937], Loss: 1.9480\n",
      "Epoch [28/100], Step [801/937], Loss: 1.8823\n",
      "Epoch [28/100], Step [901/937], Loss: 2.0461\n",
      "Epoch [29/100], Step [1/937], Loss: 1.9646\n",
      "Epoch [29/100], Step [101/937], Loss: 2.0273\n",
      "Epoch [29/100], Step [201/937], Loss: 2.1827\n",
      "Epoch [29/100], Step [301/937], Loss: 1.9410\n",
      "Epoch [29/100], Step [401/937], Loss: 1.9487\n",
      "Epoch [29/100], Step [501/937], Loss: 1.9738\n",
      "Epoch [29/100], Step [601/937], Loss: 1.8538\n",
      "Epoch [29/100], Step [701/937], Loss: 1.8319\n",
      "Epoch [29/100], Step [801/937], Loss: 1.9531\n",
      "Epoch [29/100], Step [901/937], Loss: 1.9568\n",
      "Epoch [30/100], Step [1/937], Loss: 1.9366\n",
      "Epoch [30/100], Step [101/937], Loss: 1.9760\n",
      "Epoch [30/100], Step [201/937], Loss: 1.9635\n",
      "Epoch [30/100], Step [301/937], Loss: 2.0007\n",
      "Epoch [30/100], Step [401/937], Loss: 1.9668\n",
      "Epoch [30/100], Step [501/937], Loss: 1.9446\n",
      "Epoch [30/100], Step [601/937], Loss: 1.8535\n",
      "Epoch [30/100], Step [701/937], Loss: 1.8851\n",
      "Epoch [30/100], Step [801/937], Loss: 2.0128\n",
      "Epoch [30/100], Step [901/937], Loss: 2.0292\n",
      "Epoch [31/100], Step [1/937], Loss: 1.8459\n",
      "Epoch [31/100], Step [101/937], Loss: 1.8612\n",
      "Epoch [31/100], Step [201/937], Loss: 1.9353\n",
      "Epoch [31/100], Step [301/937], Loss: 1.9326\n",
      "Epoch [31/100], Step [401/937], Loss: 1.8662\n",
      "Epoch [31/100], Step [501/937], Loss: 1.8922\n",
      "Epoch [31/100], Step [601/937], Loss: 1.8180\n",
      "Epoch [31/100], Step [701/937], Loss: 1.9755\n",
      "Epoch [31/100], Step [801/937], Loss: 1.9272\n",
      "Epoch [31/100], Step [901/937], Loss: 1.8573\n",
      "Epoch [32/100], Step [1/937], Loss: 1.8945\n",
      "Epoch [32/100], Step [101/937], Loss: 1.9712\n",
      "Epoch [32/100], Step [201/937], Loss: 2.0445\n",
      "Epoch [32/100], Step [301/937], Loss: 2.1079\n",
      "Epoch [32/100], Step [401/937], Loss: 1.9348\n",
      "Epoch [32/100], Step [501/937], Loss: 1.8156\n",
      "Epoch [32/100], Step [601/937], Loss: 2.0063\n",
      "Epoch [32/100], Step [701/937], Loss: 1.9661\n",
      "Epoch [32/100], Step [801/937], Loss: 1.8476\n",
      "Epoch [32/100], Step [901/937], Loss: 2.1075\n",
      "Epoch [33/100], Step [1/937], Loss: 2.0056\n",
      "Epoch [33/100], Step [101/937], Loss: 1.8807\n",
      "Epoch [33/100], Step [201/937], Loss: 1.8491\n",
      "Epoch [33/100], Step [301/937], Loss: 1.9542\n",
      "Epoch [33/100], Step [401/937], Loss: 2.0420\n",
      "Epoch [33/100], Step [501/937], Loss: 1.9859\n",
      "Epoch [33/100], Step [601/937], Loss: 1.9271\n",
      "Epoch [33/100], Step [701/937], Loss: 2.0313\n",
      "Epoch [33/100], Step [801/937], Loss: 1.9362\n",
      "Epoch [33/100], Step [901/937], Loss: 2.0305\n",
      "Epoch [34/100], Step [1/937], Loss: 1.9566\n",
      "Epoch [34/100], Step [101/937], Loss: 1.9557\n",
      "Epoch [34/100], Step [201/937], Loss: 1.9375\n",
      "Epoch [34/100], Step [301/937], Loss: 1.9789\n",
      "Epoch [34/100], Step [401/937], Loss: 2.0090\n",
      "Epoch [34/100], Step [501/937], Loss: 1.8625\n",
      "Epoch [34/100], Step [601/937], Loss: 1.9820\n",
      "Epoch [34/100], Step [701/937], Loss: 1.9800\n",
      "Epoch [34/100], Step [801/937], Loss: 1.9913\n",
      "Epoch [34/100], Step [901/937], Loss: 1.8480\n",
      "Epoch [35/100], Step [1/937], Loss: 1.9188\n",
      "Epoch [35/100], Step [101/937], Loss: 1.9565\n",
      "Epoch [35/100], Step [201/937], Loss: 1.9369\n",
      "Epoch [35/100], Step [301/937], Loss: 1.9055\n",
      "Epoch [35/100], Step [401/937], Loss: 2.0738\n",
      "Epoch [35/100], Step [501/937], Loss: 1.9672\n",
      "Epoch [35/100], Step [601/937], Loss: 1.9641\n",
      "Epoch [35/100], Step [701/937], Loss: 2.0441\n",
      "Epoch [35/100], Step [801/937], Loss: 1.8626\n",
      "Epoch [35/100], Step [901/937], Loss: 1.9825\n",
      "Epoch [36/100], Step [1/937], Loss: 1.9622\n",
      "Epoch [36/100], Step [101/937], Loss: 1.9239\n",
      "Epoch [36/100], Step [201/937], Loss: 1.9073\n",
      "Epoch [36/100], Step [301/937], Loss: 1.9375\n",
      "Epoch [36/100], Step [401/937], Loss: 1.9571\n",
      "Epoch [36/100], Step [501/937], Loss: 1.8639\n",
      "Epoch [36/100], Step [601/937], Loss: 2.0356\n",
      "Epoch [36/100], Step [701/937], Loss: 1.9615\n",
      "Epoch [36/100], Step [801/937], Loss: 1.9876\n",
      "Epoch [36/100], Step [901/937], Loss: 2.0285\n",
      "Epoch [37/100], Step [1/937], Loss: 1.8317\n",
      "Epoch [37/100], Step [101/937], Loss: 1.8767\n",
      "Epoch [37/100], Step [201/937], Loss: 2.0498\n",
      "Epoch [37/100], Step [301/937], Loss: 2.0939\n",
      "Epoch [37/100], Step [401/937], Loss: 1.9107\n",
      "Epoch [37/100], Step [501/937], Loss: 1.9884\n",
      "Epoch [37/100], Step [601/937], Loss: 1.9725\n",
      "Epoch [37/100], Step [701/937], Loss: 1.9830\n",
      "Epoch [37/100], Step [801/937], Loss: 1.9238\n",
      "Epoch [37/100], Step [901/937], Loss: 2.0913\n",
      "Epoch [38/100], Step [1/937], Loss: 1.9829\n",
      "Epoch [38/100], Step [101/937], Loss: 1.9790\n",
      "Epoch [38/100], Step [201/937], Loss: 1.8789\n",
      "Epoch [38/100], Step [301/937], Loss: 1.9538\n",
      "Epoch [38/100], Step [401/937], Loss: 1.9359\n",
      "Epoch [38/100], Step [501/937], Loss: 1.9678\n",
      "Epoch [38/100], Step [601/937], Loss: 1.9190\n",
      "Epoch [38/100], Step [701/937], Loss: 2.0071\n",
      "Epoch [38/100], Step [801/937], Loss: 1.9496\n",
      "Epoch [38/100], Step [901/937], Loss: 2.0908\n",
      "Epoch [39/100], Step [1/937], Loss: 2.0392\n",
      "Epoch [39/100], Step [101/937], Loss: 1.9492\n",
      "Epoch [39/100], Step [201/937], Loss: 2.0165\n",
      "Epoch [39/100], Step [301/937], Loss: 1.8776\n",
      "Epoch [39/100], Step [401/937], Loss: 1.8871\n",
      "Epoch [39/100], Step [501/937], Loss: 1.8504\n",
      "Epoch [39/100], Step [601/937], Loss: 1.9032\n",
      "Epoch [39/100], Step [701/937], Loss: 1.9823\n",
      "Epoch [39/100], Step [801/937], Loss: 1.9551\n",
      "Epoch [39/100], Step [901/937], Loss: 1.9363\n",
      "Epoch [40/100], Step [1/937], Loss: 1.8451\n",
      "Epoch [40/100], Step [101/937], Loss: 1.9691\n",
      "Epoch [40/100], Step [201/937], Loss: 1.9570\n",
      "Epoch [40/100], Step [301/937], Loss: 1.9781\n",
      "Epoch [40/100], Step [401/937], Loss: 1.9102\n",
      "Epoch [40/100], Step [501/937], Loss: 1.8407\n",
      "Epoch [40/100], Step [601/937], Loss: 2.0619\n",
      "Epoch [40/100], Step [701/937], Loss: 1.9380\n",
      "Epoch [40/100], Step [801/937], Loss: 1.9805\n",
      "Epoch [40/100], Step [901/937], Loss: 1.9890\n",
      "Epoch [41/100], Step [1/937], Loss: 1.9172\n",
      "Epoch [41/100], Step [101/937], Loss: 2.0330\n",
      "Epoch [41/100], Step [201/937], Loss: 1.8604\n",
      "Epoch [41/100], Step [301/937], Loss: 1.8467\n",
      "Epoch [41/100], Step [401/937], Loss: 1.9676\n",
      "Epoch [41/100], Step [501/937], Loss: 2.0122\n",
      "Epoch [41/100], Step [601/937], Loss: 1.9576\n",
      "Epoch [41/100], Step [701/937], Loss: 1.8780\n",
      "Epoch [41/100], Step [801/937], Loss: 1.9839\n",
      "Epoch [41/100], Step [901/937], Loss: 1.8457\n",
      "Epoch [42/100], Step [1/937], Loss: 2.0065\n",
      "Epoch [42/100], Step [101/937], Loss: 2.0619\n",
      "Epoch [42/100], Step [201/937], Loss: 1.9132\n",
      "Epoch [42/100], Step [301/937], Loss: 1.9986\n",
      "Epoch [42/100], Step [401/937], Loss: 1.8934\n",
      "Epoch [42/100], Step [501/937], Loss: 1.9072\n",
      "Epoch [42/100], Step [601/937], Loss: 1.9222\n",
      "Epoch [42/100], Step [701/937], Loss: 2.0426\n",
      "Epoch [42/100], Step [801/937], Loss: 2.0072\n",
      "Epoch [42/100], Step [901/937], Loss: 1.9841\n",
      "Epoch [43/100], Step [1/937], Loss: 1.9383\n",
      "Epoch [43/100], Step [101/937], Loss: 1.9758\n",
      "Epoch [43/100], Step [201/937], Loss: 1.9074\n",
      "Epoch [43/100], Step [301/937], Loss: 1.9915\n",
      "Epoch [43/100], Step [401/937], Loss: 1.9490\n",
      "Epoch [43/100], Step [501/937], Loss: 1.9268\n",
      "Epoch [43/100], Step [601/937], Loss: 1.9729\n",
      "Epoch [43/100], Step [701/937], Loss: 1.9803\n",
      "Epoch [43/100], Step [801/937], Loss: 1.9523\n",
      "Epoch [43/100], Step [901/937], Loss: 2.0282\n",
      "Epoch [44/100], Step [1/937], Loss: 1.9190\n",
      "Epoch [44/100], Step [101/937], Loss: 1.9366\n",
      "Epoch [44/100], Step [201/937], Loss: 1.9065\n",
      "Epoch [44/100], Step [301/937], Loss: 2.0167\n",
      "Epoch [44/100], Step [401/937], Loss: 1.8155\n",
      "Epoch [44/100], Step [501/937], Loss: 1.8966\n",
      "Epoch [44/100], Step [601/937], Loss: 2.0433\n",
      "Epoch [44/100], Step [701/937], Loss: 1.9667\n",
      "Epoch [44/100], Step [801/937], Loss: 2.0779\n",
      "Epoch [44/100], Step [901/937], Loss: 1.9395\n",
      "Epoch [45/100], Step [1/937], Loss: 1.9359\n",
      "Epoch [45/100], Step [101/937], Loss: 1.9319\n",
      "Epoch [45/100], Step [201/937], Loss: 2.0138\n",
      "Epoch [45/100], Step [301/937], Loss: 1.9233\n",
      "Epoch [45/100], Step [401/937], Loss: 1.9777\n",
      "Epoch [45/100], Step [501/937], Loss: 1.8885\n",
      "Epoch [45/100], Step [601/937], Loss: 2.0032\n",
      "Epoch [45/100], Step [701/937], Loss: 1.8777\n",
      "Epoch [45/100], Step [801/937], Loss: 2.0601\n",
      "Epoch [45/100], Step [901/937], Loss: 1.9335\n",
      "Epoch [46/100], Step [1/937], Loss: 1.9683\n",
      "Epoch [46/100], Step [101/937], Loss: 1.8724\n",
      "Epoch [46/100], Step [201/937], Loss: 1.9243\n",
      "Epoch [46/100], Step [301/937], Loss: 1.9353\n",
      "Epoch [46/100], Step [401/937], Loss: 1.8866\n",
      "Epoch [46/100], Step [501/937], Loss: 1.9290\n",
      "Epoch [46/100], Step [601/937], Loss: 1.9962\n",
      "Epoch [46/100], Step [701/937], Loss: 1.9520\n",
      "Epoch [46/100], Step [801/937], Loss: 2.0731\n",
      "Epoch [46/100], Step [901/937], Loss: 1.9787\n",
      "Epoch [47/100], Step [1/937], Loss: 1.9077\n",
      "Epoch [47/100], Step [101/937], Loss: 2.0436\n",
      "Epoch [47/100], Step [201/937], Loss: 1.8748\n",
      "Epoch [47/100], Step [301/937], Loss: 1.8707\n",
      "Epoch [47/100], Step [401/937], Loss: 1.9664\n",
      "Epoch [47/100], Step [501/937], Loss: 1.9149\n",
      "Epoch [47/100], Step [601/937], Loss: 1.8027\n",
      "Epoch [47/100], Step [701/937], Loss: 1.8939\n",
      "Epoch [47/100], Step [801/937], Loss: 1.9845\n",
      "Epoch [47/100], Step [901/937], Loss: 1.9830\n",
      "Epoch [48/100], Step [1/937], Loss: 2.0250\n",
      "Epoch [48/100], Step [101/937], Loss: 1.9833\n",
      "Epoch [48/100], Step [201/937], Loss: 1.9987\n",
      "Epoch [48/100], Step [301/937], Loss: 2.0124\n",
      "Epoch [48/100], Step [401/937], Loss: 1.9532\n",
      "Epoch [48/100], Step [501/937], Loss: 1.9767\n",
      "Epoch [48/100], Step [601/937], Loss: 1.9973\n",
      "Epoch [48/100], Step [701/937], Loss: 1.9521\n",
      "Epoch [48/100], Step [801/937], Loss: 1.9220\n",
      "Epoch [48/100], Step [901/937], Loss: 1.9533\n",
      "Epoch [49/100], Step [1/937], Loss: 1.9194\n",
      "Epoch [49/100], Step [101/937], Loss: 2.0128\n",
      "Epoch [49/100], Step [201/937], Loss: 1.9746\n",
      "Epoch [49/100], Step [301/937], Loss: 1.9773\n",
      "Epoch [49/100], Step [401/937], Loss: 1.8742\n",
      "Epoch [49/100], Step [501/937], Loss: 2.0586\n",
      "Epoch [49/100], Step [601/937], Loss: 1.9331\n",
      "Epoch [49/100], Step [701/937], Loss: 1.9683\n",
      "Epoch [49/100], Step [801/937], Loss: 1.9201\n",
      "Epoch [49/100], Step [901/937], Loss: 1.9242\n",
      "Epoch [50/100], Step [1/937], Loss: 1.7943\n",
      "Epoch [50/100], Step [101/937], Loss: 1.9745\n",
      "Epoch [50/100], Step [201/937], Loss: 2.0673\n",
      "Epoch [50/100], Step [301/937], Loss: 1.9687\n",
      "Epoch [50/100], Step [401/937], Loss: 1.9279\n",
      "Epoch [50/100], Step [501/937], Loss: 1.9713\n",
      "Epoch [50/100], Step [601/937], Loss: 1.9646\n",
      "Epoch [50/100], Step [701/937], Loss: 1.9818\n",
      "Epoch [50/100], Step [801/937], Loss: 1.9200\n",
      "Epoch [50/100], Step [901/937], Loss: 1.9338\n",
      "Epoch [51/100], Step [1/937], Loss: 1.9653\n",
      "Epoch [51/100], Step [101/937], Loss: 1.8776\n",
      "Epoch [51/100], Step [201/937], Loss: 2.0210\n",
      "Epoch [51/100], Step [301/937], Loss: 1.8827\n",
      "Epoch [51/100], Step [401/937], Loss: 1.9395\n",
      "Epoch [51/100], Step [501/937], Loss: 2.0267\n",
      "Epoch [51/100], Step [601/937], Loss: 1.9543\n",
      "Epoch [51/100], Step [701/937], Loss: 2.0005\n",
      "Epoch [51/100], Step [801/937], Loss: 1.9694\n",
      "Epoch [51/100], Step [901/937], Loss: 1.9923\n",
      "Epoch [52/100], Step [1/937], Loss: 1.9513\n",
      "Epoch [52/100], Step [101/937], Loss: 1.9134\n",
      "Epoch [52/100], Step [201/937], Loss: 1.8507\n",
      "Epoch [52/100], Step [301/937], Loss: 1.9075\n",
      "Epoch [52/100], Step [401/937], Loss: 1.9382\n",
      "Epoch [52/100], Step [501/937], Loss: 1.9121\n",
      "Epoch [52/100], Step [601/937], Loss: 2.0244\n",
      "Epoch [52/100], Step [701/937], Loss: 1.8905\n",
      "Epoch [52/100], Step [801/937], Loss: 1.9363\n",
      "Epoch [52/100], Step [901/937], Loss: 1.9301\n",
      "Epoch [53/100], Step [1/937], Loss: 1.9716\n",
      "Epoch [53/100], Step [101/937], Loss: 1.8358\n",
      "Epoch [53/100], Step [201/937], Loss: 2.0502\n",
      "Epoch [53/100], Step [301/937], Loss: 1.9756\n",
      "Epoch [53/100], Step [401/937], Loss: 1.9593\n",
      "Epoch [53/100], Step [501/937], Loss: 1.8629\n",
      "Epoch [53/100], Step [601/937], Loss: 2.0273\n",
      "Epoch [53/100], Step [701/937], Loss: 1.9542\n",
      "Epoch [53/100], Step [801/937], Loss: 1.9824\n",
      "Epoch [53/100], Step [901/937], Loss: 1.9326\n",
      "Epoch [54/100], Step [1/937], Loss: 1.9067\n",
      "Epoch [54/100], Step [101/937], Loss: 1.8795\n",
      "Epoch [54/100], Step [201/937], Loss: 2.0001\n",
      "Epoch [54/100], Step [301/937], Loss: 1.8404\n",
      "Epoch [54/100], Step [401/937], Loss: 1.9038\n",
      "Epoch [54/100], Step [501/937], Loss: 1.9367\n",
      "Epoch [54/100], Step [601/937], Loss: 1.8902\n",
      "Epoch [54/100], Step [701/937], Loss: 1.9043\n",
      "Epoch [54/100], Step [801/937], Loss: 2.0072\n",
      "Epoch [54/100], Step [901/937], Loss: 1.9963\n",
      "Epoch [55/100], Step [1/937], Loss: 2.0151\n",
      "Epoch [55/100], Step [101/937], Loss: 2.0289\n",
      "Epoch [55/100], Step [201/937], Loss: 1.9052\n",
      "Epoch [55/100], Step [301/937], Loss: 1.9037\n",
      "Epoch [55/100], Step [401/937], Loss: 2.0460\n",
      "Epoch [55/100], Step [501/937], Loss: 1.8912\n",
      "Epoch [55/100], Step [601/937], Loss: 1.9897\n",
      "Epoch [55/100], Step [701/937], Loss: 1.9819\n",
      "Epoch [55/100], Step [801/937], Loss: 1.9986\n",
      "Epoch [55/100], Step [901/937], Loss: 2.0309\n",
      "Epoch [56/100], Step [1/937], Loss: 1.9848\n",
      "Epoch [56/100], Step [101/937], Loss: 1.9755\n",
      "Epoch [56/100], Step [201/937], Loss: 1.9474\n",
      "Epoch [56/100], Step [301/937], Loss: 1.9689\n",
      "Epoch [56/100], Step [401/937], Loss: 1.9375\n",
      "Epoch [56/100], Step [501/937], Loss: 2.0453\n",
      "Epoch [56/100], Step [601/937], Loss: 2.0283\n",
      "Epoch [56/100], Step [701/937], Loss: 1.9530\n",
      "Epoch [56/100], Step [801/937], Loss: 1.9375\n",
      "Epoch [56/100], Step [901/937], Loss: 1.9134\n",
      "Epoch [57/100], Step [1/937], Loss: 1.9509\n",
      "Epoch [57/100], Step [101/937], Loss: 1.9197\n",
      "Epoch [57/100], Step [201/937], Loss: 2.0124\n",
      "Epoch [57/100], Step [301/937], Loss: 1.9406\n",
      "Epoch [57/100], Step [401/937], Loss: 2.0373\n",
      "Epoch [57/100], Step [501/937], Loss: 1.9569\n",
      "Epoch [57/100], Step [601/937], Loss: 1.9726\n",
      "Epoch [57/100], Step [701/937], Loss: 2.0113\n",
      "Epoch [57/100], Step [801/937], Loss: 2.0179\n",
      "Epoch [57/100], Step [901/937], Loss: 2.0118\n",
      "Epoch [58/100], Step [1/937], Loss: 1.9071\n",
      "Epoch [58/100], Step [101/937], Loss: 1.9063\n",
      "Epoch [58/100], Step [201/937], Loss: 2.0328\n",
      "Epoch [58/100], Step [301/937], Loss: 1.9353\n",
      "Epoch [58/100], Step [401/937], Loss: 1.9086\n",
      "Epoch [58/100], Step [501/937], Loss: 1.9370\n",
      "Epoch [58/100], Step [601/937], Loss: 1.8928\n",
      "Epoch [58/100], Step [701/937], Loss: 1.8725\n",
      "Epoch [58/100], Step [801/937], Loss: 1.9443\n",
      "Epoch [58/100], Step [901/937], Loss: 1.9536\n",
      "Epoch [59/100], Step [1/937], Loss: 2.0249\n",
      "Epoch [59/100], Step [101/937], Loss: 1.9800\n",
      "Epoch [59/100], Step [201/937], Loss: 1.9679\n",
      "Epoch [59/100], Step [301/937], Loss: 1.8895\n",
      "Epoch [59/100], Step [401/937], Loss: 1.9960\n",
      "Epoch [59/100], Step [501/937], Loss: 1.9219\n",
      "Epoch [59/100], Step [601/937], Loss: 1.8205\n",
      "Epoch [59/100], Step [701/937], Loss: 1.9215\n",
      "Epoch [59/100], Step [801/937], Loss: 1.9583\n",
      "Epoch [59/100], Step [901/937], Loss: 1.9514\n",
      "Epoch [60/100], Step [1/937], Loss: 1.8626\n",
      "Epoch [60/100], Step [101/937], Loss: 1.9479\n",
      "Epoch [60/100], Step [201/937], Loss: 1.8936\n",
      "Epoch [60/100], Step [301/937], Loss: 1.9524\n",
      "Epoch [60/100], Step [401/937], Loss: 1.9515\n",
      "Epoch [60/100], Step [501/937], Loss: 1.9449\n",
      "Epoch [60/100], Step [601/937], Loss: 1.9370\n",
      "Epoch [60/100], Step [701/937], Loss: 1.8924\n",
      "Epoch [60/100], Step [801/937], Loss: 2.0035\n",
      "Epoch [60/100], Step [901/937], Loss: 1.8456\n",
      "Epoch [61/100], Step [1/937], Loss: 2.0003\n",
      "Epoch [61/100], Step [101/937], Loss: 1.9824\n",
      "Epoch [61/100], Step [201/937], Loss: 1.8791\n",
      "Epoch [61/100], Step [301/937], Loss: 1.9058\n",
      "Epoch [61/100], Step [401/937], Loss: 1.9367\n",
      "Epoch [61/100], Step [501/937], Loss: 1.8617\n",
      "Epoch [61/100], Step [601/937], Loss: 1.9371\n",
      "Epoch [61/100], Step [701/937], Loss: 1.9275\n",
      "Epoch [61/100], Step [801/937], Loss: 1.9468\n",
      "Epoch [61/100], Step [901/937], Loss: 1.8996\n",
      "Epoch [62/100], Step [1/937], Loss: 1.9985\n",
      "Epoch [62/100], Step [101/937], Loss: 1.9211\n",
      "Epoch [62/100], Step [201/937], Loss: 1.9680\n",
      "Epoch [62/100], Step [301/937], Loss: 1.9221\n",
      "Epoch [62/100], Step [401/937], Loss: 1.9402\n",
      "Epoch [62/100], Step [501/937], Loss: 1.9204\n",
      "Epoch [62/100], Step [601/937], Loss: 1.9488\n",
      "Epoch [62/100], Step [701/937], Loss: 1.9190\n",
      "Epoch [62/100], Step [801/937], Loss: 1.8798\n",
      "Epoch [62/100], Step [901/937], Loss: 1.9659\n",
      "Epoch [63/100], Step [1/937], Loss: 1.9319\n",
      "Epoch [63/100], Step [101/937], Loss: 1.9071\n",
      "Epoch [63/100], Step [201/937], Loss: 1.9864\n",
      "Epoch [63/100], Step [301/937], Loss: 1.9524\n",
      "Epoch [63/100], Step [401/937], Loss: 1.9420\n",
      "Epoch [63/100], Step [501/937], Loss: 1.8940\n",
      "Epoch [63/100], Step [601/937], Loss: 2.0140\n",
      "Epoch [63/100], Step [701/937], Loss: 1.9099\n",
      "Epoch [63/100], Step [801/937], Loss: 1.9167\n",
      "Epoch [63/100], Step [901/937], Loss: 2.0278\n",
      "Epoch [64/100], Step [1/937], Loss: 1.9037\n",
      "Epoch [64/100], Step [101/937], Loss: 1.9883\n",
      "Epoch [64/100], Step [201/937], Loss: 1.8099\n",
      "Epoch [64/100], Step [301/937], Loss: 2.0887\n",
      "Epoch [64/100], Step [401/937], Loss: 1.8468\n",
      "Epoch [64/100], Step [501/937], Loss: 1.9983\n",
      "Epoch [64/100], Step [601/937], Loss: 1.8031\n",
      "Epoch [64/100], Step [701/937], Loss: 1.9092\n",
      "Epoch [64/100], Step [801/937], Loss: 1.8585\n",
      "Epoch [64/100], Step [901/937], Loss: 1.9095\n",
      "Epoch [65/100], Step [1/937], Loss: 2.0152\n",
      "Epoch [65/100], Step [101/937], Loss: 1.8569\n",
      "Epoch [65/100], Step [201/937], Loss: 1.9384\n",
      "Epoch [65/100], Step [301/937], Loss: 1.7985\n",
      "Epoch [65/100], Step [401/937], Loss: 1.9858\n",
      "Epoch [65/100], Step [501/937], Loss: 1.9682\n",
      "Epoch [65/100], Step [601/937], Loss: 2.0049\n",
      "Epoch [65/100], Step [701/937], Loss: 1.9196\n",
      "Epoch [65/100], Step [801/937], Loss: 1.9098\n",
      "Epoch [65/100], Step [901/937], Loss: 1.8775\n",
      "Epoch [66/100], Step [1/937], Loss: 1.8648\n",
      "Epoch [66/100], Step [101/937], Loss: 1.9191\n",
      "Epoch [66/100], Step [201/937], Loss: 1.8750\n",
      "Epoch [66/100], Step [301/937], Loss: 2.0011\n",
      "Epoch [66/100], Step [401/937], Loss: 1.8320\n",
      "Epoch [66/100], Step [501/937], Loss: 1.9356\n",
      "Epoch [66/100], Step [601/937], Loss: 1.9348\n",
      "Epoch [66/100], Step [701/937], Loss: 1.9321\n",
      "Epoch [66/100], Step [801/937], Loss: 1.8311\n",
      "Epoch [66/100], Step [901/937], Loss: 1.9712\n",
      "Epoch [67/100], Step [1/937], Loss: 1.9061\n",
      "Epoch [67/100], Step [101/937], Loss: 1.9818\n",
      "Epoch [67/100], Step [201/937], Loss: 1.9345\n",
      "Epoch [67/100], Step [301/937], Loss: 1.8567\n",
      "Epoch [67/100], Step [401/937], Loss: 1.9355\n",
      "Epoch [67/100], Step [501/937], Loss: 1.9650\n",
      "Epoch [67/100], Step [601/937], Loss: 1.9023\n",
      "Epoch [67/100], Step [701/937], Loss: 1.9832\n",
      "Epoch [67/100], Step [801/937], Loss: 1.9500\n",
      "Epoch [67/100], Step [901/937], Loss: 1.8926\n",
      "Epoch [68/100], Step [1/937], Loss: 1.9342\n",
      "Epoch [68/100], Step [101/937], Loss: 1.8898\n",
      "Epoch [68/100], Step [201/937], Loss: 1.9349\n",
      "Epoch [68/100], Step [301/937], Loss: 1.9508\n",
      "Epoch [68/100], Step [401/937], Loss: 1.9651\n",
      "Epoch [68/100], Step [501/937], Loss: 1.9385\n",
      "Epoch [68/100], Step [601/937], Loss: 1.9685\n",
      "Epoch [68/100], Step [701/937], Loss: 1.9340\n",
      "Epoch [68/100], Step [801/937], Loss: 1.9217\n",
      "Epoch [68/100], Step [901/937], Loss: 1.9511\n",
      "Epoch [69/100], Step [1/937], Loss: 1.9717\n",
      "Epoch [69/100], Step [101/937], Loss: 1.9841\n",
      "Epoch [69/100], Step [201/937], Loss: 2.0546\n",
      "Epoch [69/100], Step [301/937], Loss: 1.9038\n",
      "Epoch [69/100], Step [401/937], Loss: 1.9237\n",
      "Epoch [69/100], Step [501/937], Loss: 1.8896\n",
      "Epoch [69/100], Step [601/937], Loss: 1.8436\n",
      "Epoch [69/100], Step [701/937], Loss: 1.9358\n",
      "Epoch [69/100], Step [801/937], Loss: 1.9997\n",
      "Epoch [69/100], Step [901/937], Loss: 1.9158\n",
      "Epoch [70/100], Step [1/937], Loss: 1.9460\n",
      "Epoch [70/100], Step [101/937], Loss: 1.9060\n",
      "Epoch [70/100], Step [201/937], Loss: 1.9284\n",
      "Epoch [70/100], Step [301/937], Loss: 2.0886\n",
      "Epoch [70/100], Step [401/937], Loss: 1.8747\n",
      "Epoch [70/100], Step [501/937], Loss: 1.8888\n",
      "Epoch [70/100], Step [601/937], Loss: 1.9185\n",
      "Epoch [70/100], Step [701/937], Loss: 1.8933\n",
      "Epoch [70/100], Step [801/937], Loss: 1.9830\n",
      "Epoch [70/100], Step [901/937], Loss: 1.9054\n",
      "Epoch [71/100], Step [1/937], Loss: 1.8905\n",
      "Epoch [71/100], Step [101/937], Loss: 1.8422\n",
      "Epoch [71/100], Step [201/937], Loss: 1.9653\n",
      "Epoch [71/100], Step [301/937], Loss: 2.0666\n",
      "Epoch [71/100], Step [401/937], Loss: 1.8738\n",
      "Epoch [71/100], Step [501/937], Loss: 2.0416\n",
      "Epoch [71/100], Step [601/937], Loss: 1.9704\n",
      "Epoch [71/100], Step [701/937], Loss: 1.8976\n",
      "Epoch [71/100], Step [801/937], Loss: 1.8773\n",
      "Epoch [71/100], Step [901/937], Loss: 2.0121\n",
      "Epoch [72/100], Step [1/937], Loss: 1.9245\n",
      "Epoch [72/100], Step [101/937], Loss: 1.9036\n",
      "Epoch [72/100], Step [201/937], Loss: 1.8968\n",
      "Epoch [72/100], Step [301/937], Loss: 1.9492\n",
      "Epoch [72/100], Step [401/937], Loss: 1.9684\n",
      "Epoch [72/100], Step [501/937], Loss: 1.8910\n",
      "Epoch [72/100], Step [601/937], Loss: 2.0092\n",
      "Epoch [72/100], Step [701/937], Loss: 1.8908\n",
      "Epoch [72/100], Step [801/937], Loss: 2.0261\n",
      "Epoch [72/100], Step [901/937], Loss: 1.8121\n",
      "Epoch [73/100], Step [1/937], Loss: 1.9394\n",
      "Epoch [73/100], Step [101/937], Loss: 1.9271\n",
      "Epoch [73/100], Step [201/937], Loss: 1.9222\n",
      "Epoch [73/100], Step [301/937], Loss: 1.9803\n",
      "Epoch [73/100], Step [401/937], Loss: 2.0257\n",
      "Epoch [73/100], Step [501/937], Loss: 1.9228\n",
      "Epoch [73/100], Step [601/937], Loss: 2.0104\n",
      "Epoch [73/100], Step [701/937], Loss: 1.8954\n",
      "Epoch [73/100], Step [801/937], Loss: 1.9149\n",
      "Epoch [73/100], Step [901/937], Loss: 1.9701\n",
      "Epoch [74/100], Step [1/937], Loss: 1.9972\n",
      "Epoch [74/100], Step [101/937], Loss: 1.9158\n",
      "Epoch [74/100], Step [201/937], Loss: 2.0301\n",
      "Epoch [74/100], Step [301/937], Loss: 1.9672\n",
      "Epoch [74/100], Step [401/937], Loss: 1.9777\n",
      "Epoch [74/100], Step [501/937], Loss: 1.8746\n",
      "Epoch [74/100], Step [601/937], Loss: 1.9537\n",
      "Epoch [74/100], Step [701/937], Loss: 1.9201\n",
      "Epoch [74/100], Step [801/937], Loss: 2.0141\n",
      "Epoch [74/100], Step [901/937], Loss: 1.8632\n",
      "Epoch [75/100], Step [1/937], Loss: 1.9263\n",
      "Epoch [75/100], Step [101/937], Loss: 1.9850\n",
      "Epoch [75/100], Step [201/937], Loss: 1.9380\n",
      "Epoch [75/100], Step [301/937], Loss: 1.9832\n",
      "Epoch [75/100], Step [401/937], Loss: 1.9991\n",
      "Epoch [75/100], Step [501/937], Loss: 1.8460\n",
      "Epoch [75/100], Step [601/937], Loss: 1.9810\n",
      "Epoch [75/100], Step [701/937], Loss: 1.8202\n",
      "Epoch [75/100], Step [801/937], Loss: 1.9087\n",
      "Epoch [75/100], Step [901/937], Loss: 1.9810\n",
      "Epoch [76/100], Step [1/937], Loss: 2.0017\n",
      "Epoch [76/100], Step [101/937], Loss: 1.9888\n",
      "Epoch [76/100], Step [201/937], Loss: 1.9200\n",
      "Epoch [76/100], Step [301/937], Loss: 1.9047\n",
      "Epoch [76/100], Step [401/937], Loss: 2.0345\n",
      "Epoch [76/100], Step [501/937], Loss: 1.9955\n",
      "Epoch [76/100], Step [601/937], Loss: 1.9544\n",
      "Epoch [76/100], Step [701/937], Loss: 2.0140\n",
      "Epoch [76/100], Step [801/937], Loss: 1.9878\n",
      "Epoch [76/100], Step [901/937], Loss: 1.9285\n",
      "Epoch [77/100], Step [1/937], Loss: 1.8898\n",
      "Epoch [77/100], Step [101/937], Loss: 1.8839\n",
      "Epoch [77/100], Step [201/937], Loss: 1.9673\n",
      "Epoch [77/100], Step [301/937], Loss: 1.9975\n",
      "Epoch [77/100], Step [401/937], Loss: 1.9302\n",
      "Epoch [77/100], Step [501/937], Loss: 1.9654\n",
      "Epoch [77/100], Step [601/937], Loss: 1.8879\n",
      "Epoch [77/100], Step [701/937], Loss: 1.8283\n",
      "Epoch [77/100], Step [801/937], Loss: 1.8757\n",
      "Epoch [77/100], Step [901/937], Loss: 1.8319\n",
      "Epoch [78/100], Step [1/937], Loss: 1.9381\n",
      "Epoch [78/100], Step [101/937], Loss: 1.8924\n",
      "Epoch [78/100], Step [201/937], Loss: 2.0301\n",
      "Epoch [78/100], Step [301/937], Loss: 1.8771\n",
      "Epoch [78/100], Step [401/937], Loss: 2.0206\n",
      "Epoch [78/100], Step [501/937], Loss: 1.9507\n",
      "Epoch [78/100], Step [601/937], Loss: 1.9367\n",
      "Epoch [78/100], Step [701/937], Loss: 1.9553\n",
      "Epoch [78/100], Step [801/937], Loss: 1.8749\n",
      "Epoch [78/100], Step [901/937], Loss: 2.0027\n",
      "Epoch [79/100], Step [1/937], Loss: 1.8742\n",
      "Epoch [79/100], Step [101/937], Loss: 1.9727\n",
      "Epoch [79/100], Step [201/937], Loss: 1.9512\n",
      "Epoch [79/100], Step [301/937], Loss: 1.9031\n",
      "Epoch [79/100], Step [401/937], Loss: 2.1153\n",
      "Epoch [79/100], Step [501/937], Loss: 1.9513\n",
      "Epoch [79/100], Step [601/937], Loss: 1.9625\n",
      "Epoch [79/100], Step [701/937], Loss: 1.9384\n",
      "Epoch [79/100], Step [801/937], Loss: 1.9711\n",
      "Epoch [79/100], Step [901/937], Loss: 1.9372\n",
      "Epoch [80/100], Step [1/937], Loss: 1.9206\n",
      "Epoch [80/100], Step [101/937], Loss: 1.9487\n",
      "Epoch [80/100], Step [201/937], Loss: 2.0210\n",
      "Epoch [80/100], Step [301/937], Loss: 1.8793\n",
      "Epoch [80/100], Step [401/937], Loss: 1.9805\n",
      "Epoch [80/100], Step [501/937], Loss: 1.9699\n",
      "Epoch [80/100], Step [601/937], Loss: 2.0294\n",
      "Epoch [80/100], Step [701/937], Loss: 1.9515\n",
      "Epoch [80/100], Step [801/937], Loss: 1.9090\n",
      "Epoch [80/100], Step [901/937], Loss: 2.0736\n",
      "Epoch [81/100], Step [1/937], Loss: 2.0134\n",
      "Epoch [81/100], Step [101/937], Loss: 1.9407\n",
      "Epoch [81/100], Step [201/937], Loss: 1.9285\n",
      "Epoch [81/100], Step [301/937], Loss: 1.9684\n",
      "Epoch [81/100], Step [401/937], Loss: 1.9261\n",
      "Epoch [81/100], Step [501/937], Loss: 1.8882\n",
      "Epoch [81/100], Step [601/937], Loss: 1.9068\n",
      "Epoch [81/100], Step [701/937], Loss: 1.9647\n",
      "Epoch [81/100], Step [801/937], Loss: 1.8252\n",
      "Epoch [81/100], Step [901/937], Loss: 1.8554\n",
      "Epoch [82/100], Step [1/937], Loss: 1.8960\n",
      "Epoch [82/100], Step [101/937], Loss: 1.9940\n",
      "Epoch [82/100], Step [201/937], Loss: 1.8585\n",
      "Epoch [82/100], Step [301/937], Loss: 1.9603\n",
      "Epoch [82/100], Step [401/937], Loss: 1.9819\n",
      "Epoch [82/100], Step [501/937], Loss: 1.9664\n",
      "Epoch [82/100], Step [601/937], Loss: 1.8469\n",
      "Epoch [82/100], Step [701/937], Loss: 1.9253\n",
      "Epoch [82/100], Step [801/937], Loss: 1.9347\n",
      "Epoch [82/100], Step [901/937], Loss: 1.9540\n",
      "Epoch [83/100], Step [1/937], Loss: 1.9395\n",
      "Epoch [83/100], Step [101/937], Loss: 1.9293\n",
      "Epoch [83/100], Step [201/937], Loss: 1.8773\n",
      "Epoch [83/100], Step [301/937], Loss: 1.9193\n",
      "Epoch [83/100], Step [401/937], Loss: 1.9446\n",
      "Epoch [83/100], Step [501/937], Loss: 1.9699\n",
      "Epoch [83/100], Step [601/937], Loss: 1.8610\n",
      "Epoch [83/100], Step [701/937], Loss: 2.0129\n",
      "Epoch [83/100], Step [801/937], Loss: 1.9086\n",
      "Epoch [83/100], Step [901/937], Loss: 1.9390\n",
      "Epoch [84/100], Step [1/937], Loss: 2.1062\n",
      "Epoch [84/100], Step [101/937], Loss: 1.9335\n",
      "Epoch [84/100], Step [201/937], Loss: 2.1321\n",
      "Epoch [84/100], Step [301/937], Loss: 1.9811\n",
      "Epoch [84/100], Step [401/937], Loss: 1.9987\n",
      "Epoch [84/100], Step [501/937], Loss: 1.9341\n",
      "Epoch [84/100], Step [601/937], Loss: 1.9052\n",
      "Epoch [84/100], Step [701/937], Loss: 1.9237\n",
      "Epoch [84/100], Step [801/937], Loss: 2.0120\n",
      "Epoch [84/100], Step [901/937], Loss: 1.8725\n",
      "Epoch [85/100], Step [1/937], Loss: 1.9598\n",
      "Epoch [85/100], Step [101/937], Loss: 1.9054\n",
      "Epoch [85/100], Step [201/937], Loss: 1.8901\n",
      "Epoch [85/100], Step [301/937], Loss: 1.9811\n",
      "Epoch [85/100], Step [401/937], Loss: 1.9709\n",
      "Epoch [85/100], Step [501/937], Loss: 1.9101\n",
      "Epoch [85/100], Step [601/937], Loss: 1.9767\n",
      "Epoch [85/100], Step [701/937], Loss: 1.9405\n",
      "Epoch [85/100], Step [801/937], Loss: 1.9269\n",
      "Epoch [85/100], Step [901/937], Loss: 1.9342\n",
      "Epoch [86/100], Step [1/937], Loss: 1.8939\n",
      "Epoch [86/100], Step [101/937], Loss: 1.8996\n",
      "Epoch [86/100], Step [201/937], Loss: 1.8934\n",
      "Epoch [86/100], Step [301/937], Loss: 1.9280\n",
      "Epoch [86/100], Step [401/937], Loss: 1.9835\n",
      "Epoch [86/100], Step [501/937], Loss: 1.8241\n",
      "Epoch [86/100], Step [601/937], Loss: 1.8303\n",
      "Epoch [86/100], Step [701/937], Loss: 1.9386\n",
      "Epoch [86/100], Step [801/937], Loss: 1.9675\n",
      "Epoch [86/100], Step [901/937], Loss: 1.9252\n",
      "Epoch [87/100], Step [1/937], Loss: 2.0132\n",
      "Epoch [87/100], Step [101/937], Loss: 2.0909\n",
      "Epoch [87/100], Step [201/937], Loss: 2.0249\n",
      "Epoch [87/100], Step [301/937], Loss: 1.9806\n",
      "Epoch [87/100], Step [401/937], Loss: 1.8743\n",
      "Epoch [87/100], Step [501/937], Loss: 1.9801\n",
      "Epoch [87/100], Step [601/937], Loss: 1.8340\n",
      "Epoch [87/100], Step [701/937], Loss: 2.0030\n",
      "Epoch [87/100], Step [801/937], Loss: 1.9719\n",
      "Epoch [87/100], Step [901/937], Loss: 1.9223\n",
      "Epoch [88/100], Step [1/937], Loss: 1.8287\n",
      "Epoch [88/100], Step [101/937], Loss: 1.9096\n",
      "Epoch [88/100], Step [201/937], Loss: 1.9362\n",
      "Epoch [88/100], Step [301/937], Loss: 1.8960\n",
      "Epoch [88/100], Step [401/937], Loss: 1.9222\n",
      "Epoch [88/100], Step [501/937], Loss: 1.9074\n",
      "Epoch [88/100], Step [601/937], Loss: 1.9791\n",
      "Epoch [88/100], Step [701/937], Loss: 1.9717\n",
      "Epoch [88/100], Step [801/937], Loss: 2.0274\n",
      "Epoch [88/100], Step [901/937], Loss: 1.8129\n",
      "Epoch [89/100], Step [1/937], Loss: 1.8140\n",
      "Epoch [89/100], Step [101/937], Loss: 1.9652\n",
      "Epoch [89/100], Step [201/937], Loss: 1.8647\n",
      "Epoch [89/100], Step [301/937], Loss: 1.8606\n",
      "Epoch [89/100], Step [401/937], Loss: 1.9042\n",
      "Epoch [89/100], Step [501/937], Loss: 1.9808\n",
      "Epoch [89/100], Step [601/937], Loss: 1.9335\n",
      "Epoch [89/100], Step [701/937], Loss: 1.8964\n",
      "Epoch [89/100], Step [801/937], Loss: 1.9119\n",
      "Epoch [89/100], Step [901/937], Loss: 2.0301\n",
      "Epoch [90/100], Step [1/937], Loss: 1.9373\n",
      "Epoch [90/100], Step [101/937], Loss: 1.8970\n",
      "Epoch [90/100], Step [201/937], Loss: 1.9701\n",
      "Epoch [90/100], Step [301/937], Loss: 1.8175\n",
      "Epoch [90/100], Step [401/937], Loss: 1.9390\n",
      "Epoch [90/100], Step [501/937], Loss: 1.8442\n",
      "Epoch [90/100], Step [601/937], Loss: 1.9057\n",
      "Epoch [90/100], Step [701/937], Loss: 1.8948\n",
      "Epoch [90/100], Step [801/937], Loss: 1.9138\n",
      "Epoch [90/100], Step [901/937], Loss: 1.9497\n",
      "Epoch [91/100], Step [1/937], Loss: 1.9012\n",
      "Epoch [91/100], Step [101/937], Loss: 1.9888\n",
      "Epoch [91/100], Step [201/937], Loss: 1.8582\n",
      "Epoch [91/100], Step [301/937], Loss: 1.8367\n",
      "Epoch [91/100], Step [401/937], Loss: 1.9310\n",
      "Epoch [91/100], Step [501/937], Loss: 1.9729\n",
      "Epoch [91/100], Step [601/937], Loss: 1.9152\n",
      "Epoch [91/100], Step [701/937], Loss: 1.9648\n",
      "Epoch [91/100], Step [801/937], Loss: 1.9796\n",
      "Epoch [91/100], Step [901/937], Loss: 1.9732\n",
      "Epoch [92/100], Step [1/937], Loss: 2.0184\n",
      "Epoch [92/100], Step [101/937], Loss: 1.8658\n",
      "Epoch [92/100], Step [201/937], Loss: 1.9792\n",
      "Epoch [92/100], Step [301/937], Loss: 2.0267\n",
      "Epoch [92/100], Step [401/937], Loss: 1.9670\n",
      "Epoch [92/100], Step [501/937], Loss: 1.9659\n",
      "Epoch [92/100], Step [601/937], Loss: 1.8052\n",
      "Epoch [92/100], Step [701/937], Loss: 1.8301\n",
      "Epoch [92/100], Step [801/937], Loss: 1.9417\n",
      "Epoch [92/100], Step [901/937], Loss: 1.9655\n",
      "Epoch [93/100], Step [1/937], Loss: 1.9554\n",
      "Epoch [93/100], Step [101/937], Loss: 1.8820\n",
      "Epoch [93/100], Step [201/937], Loss: 2.0596\n",
      "Epoch [93/100], Step [301/937], Loss: 1.9608\n",
      "Epoch [93/100], Step [401/937], Loss: 1.9369\n",
      "Epoch [93/100], Step [501/937], Loss: 1.9196\n",
      "Epoch [93/100], Step [601/937], Loss: 1.8440\n",
      "Epoch [93/100], Step [701/937], Loss: 1.8800\n",
      "Epoch [93/100], Step [801/937], Loss: 1.9178\n",
      "Epoch [93/100], Step [901/937], Loss: 1.8872\n",
      "Epoch [94/100], Step [1/937], Loss: 1.8508\n",
      "Epoch [94/100], Step [101/937], Loss: 1.9162\n",
      "Epoch [94/100], Step [201/937], Loss: 1.8643\n",
      "Epoch [94/100], Step [301/937], Loss: 1.9988\n",
      "Epoch [94/100], Step [401/937], Loss: 1.9219\n",
      "Epoch [94/100], Step [501/937], Loss: 2.0513\n",
      "Epoch [94/100], Step [601/937], Loss: 1.9955\n",
      "Epoch [94/100], Step [701/937], Loss: 1.9192\n",
      "Epoch [94/100], Step [801/937], Loss: 1.9063\n",
      "Epoch [94/100], Step [901/937], Loss: 2.0325\n",
      "Epoch [95/100], Step [1/937], Loss: 1.9675\n",
      "Epoch [95/100], Step [101/937], Loss: 1.9676\n",
      "Epoch [95/100], Step [201/937], Loss: 1.8659\n",
      "Epoch [95/100], Step [301/937], Loss: 1.9226\n",
      "Epoch [95/100], Step [401/937], Loss: 1.8451\n",
      "Epoch [95/100], Step [501/937], Loss: 1.9590\n",
      "Epoch [95/100], Step [601/937], Loss: 1.9822\n",
      "Epoch [95/100], Step [701/937], Loss: 1.9260\n",
      "Epoch [95/100], Step [801/937], Loss: 1.8897\n",
      "Epoch [95/100], Step [901/937], Loss: 2.0415\n",
      "Epoch [96/100], Step [1/937], Loss: 1.8623\n",
      "Epoch [96/100], Step [101/937], Loss: 1.9242\n",
      "Epoch [96/100], Step [201/937], Loss: 2.0029\n",
      "Epoch [96/100], Step [301/937], Loss: 1.8617\n",
      "Epoch [96/100], Step [401/937], Loss: 1.9405\n",
      "Epoch [96/100], Step [501/937], Loss: 1.9813\n",
      "Epoch [96/100], Step [601/937], Loss: 1.9676\n",
      "Epoch [96/100], Step [701/937], Loss: 1.9512\n",
      "Epoch [96/100], Step [801/937], Loss: 1.9879\n",
      "Epoch [96/100], Step [901/937], Loss: 2.0164\n",
      "Epoch [97/100], Step [1/937], Loss: 1.9786\n",
      "Epoch [97/100], Step [101/937], Loss: 2.0552\n",
      "Epoch [97/100], Step [201/937], Loss: 1.8664\n",
      "Epoch [97/100], Step [301/937], Loss: 1.8618\n",
      "Epoch [97/100], Step [401/937], Loss: 1.8266\n",
      "Epoch [97/100], Step [501/937], Loss: 1.8738\n",
      "Epoch [97/100], Step [601/937], Loss: 1.8905\n",
      "Epoch [97/100], Step [701/937], Loss: 1.8139\n",
      "Epoch [97/100], Step [801/937], Loss: 1.9724\n",
      "Epoch [97/100], Step [901/937], Loss: 1.9514\n",
      "Epoch [98/100], Step [1/937], Loss: 1.8596\n",
      "Epoch [98/100], Step [101/937], Loss: 1.9217\n",
      "Epoch [98/100], Step [201/937], Loss: 1.8636\n",
      "Epoch [98/100], Step [301/937], Loss: 1.9574\n",
      "Epoch [98/100], Step [401/937], Loss: 2.0593\n",
      "Epoch [98/100], Step [501/937], Loss: 1.8743\n",
      "Epoch [98/100], Step [601/937], Loss: 1.8170\n",
      "Epoch [98/100], Step [701/937], Loss: 1.8602\n",
      "Epoch [98/100], Step [801/937], Loss: 1.9400\n",
      "Epoch [98/100], Step [901/937], Loss: 1.9417\n",
      "Epoch [99/100], Step [1/937], Loss: 1.9606\n",
      "Epoch [99/100], Step [101/937], Loss: 1.9822\n",
      "Epoch [99/100], Step [201/937], Loss: 1.9801\n",
      "Epoch [99/100], Step [301/937], Loss: 1.9338\n",
      "Epoch [99/100], Step [401/937], Loss: 1.8426\n",
      "Epoch [99/100], Step [501/937], Loss: 1.9054\n",
      "Epoch [99/100], Step [601/937], Loss: 1.8895\n",
      "Epoch [99/100], Step [701/937], Loss: 2.0134\n",
      "Epoch [99/100], Step [801/937], Loss: 1.9780\n",
      "Epoch [99/100], Step [901/937], Loss: 1.8778\n",
      "Epoch [100/100], Step [1/937], Loss: 1.9961\n",
      "Epoch [100/100], Step [101/937], Loss: 1.8885\n",
      "Epoch [100/100], Step [201/937], Loss: 1.9150\n",
      "Epoch [100/100], Step [301/937], Loss: 1.9968\n",
      "Epoch [100/100], Step [401/937], Loss: 1.8958\n",
      "Epoch [100/100], Step [501/937], Loss: 1.9856\n",
      "Epoch [100/100], Step [601/937], Loss: 1.8640\n",
      "Epoch [100/100], Step [701/937], Loss: 1.8917\n",
      "Epoch [100/100], Step [801/937], Loss: 1.8454\n",
      "Epoch [100/100], Step [901/937], Loss: 2.0728\n",
      "Total training time: 462.3036825656891 seconds.\n"
     ]
    }
   ],
   "source": [
    "# CNN Training\n",
    "\n",
    "num_of_iterations = X_train.shape[0] // batch_size\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        images = torch._cast_Float(images)\n",
    "        images = images.view(images.shape[0], -1, X_train.shape[-2], X_train.shape[-1])\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = cnn_model.forward(images)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = error(outputs, labels)\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{total_epoch}], Step [{i + 1}/{num_of_iterations}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "training_end_time = time.time()\n",
    "\n",
    "print(f\"Total training time: {(training_end_time - training_start_time)} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f453402",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88b8d8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic: 2051, Number of Images: 10000, Rows: 28, Columns: 28\n",
      "Magic: 2049, Number of Labels: 10000\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_mnist_images_manual(\"dataset/t10k-images.idx3-ubyte\", \"dataset/t10k-labels.idx1-ubyte\")\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91d5c897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test Shape: torch.Size([10000, 28, 28])\n",
      "y_test Shape: tensor([7, 2, 1,  ..., 4, 5, 6], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_test Shape: {X_test.shape}\")\n",
    "print(f\"y_test Shape: {y_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ce1466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(X_test, y_test)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4685755f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct predictions: 5062 \n",
      " Total Samples: 10000\n",
      "Accuracy: 0.5062\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(test_dataloader):\n",
    "        images = torch._cast_Float(images)\n",
    "        images = images.view(images.shape[0], -1, X_test.shape[-2], X_test.shape[-1])\n",
    "        \n",
    "        outputs = cnn_model.forward(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        total_correct += (predictions == labels).sum().item()\n",
    "        total_samples += labels.shape[0]\n",
    "\n",
    "print(f\"Total correct predictions: {total_correct} \\n Total Samples: {total_samples}\")\n",
    "print(f\"Accuracy: {total_correct / total_samples}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
